{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Processing Sequences Using RNNs and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter we will cover Recurrent Neural Networks, especially useful with time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neurons and Layers\n",
    "\n",
    "A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](images/15.RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the weight vectors for inputs $w_x$ and the ones for outputs $w_y$. We can put all these vectors in two matrices $W_x$ and $W_y$. \n",
    "\n",
    "The output vector for the layer would therefore be ($b$ = bias vector; $\\phi$ = activation function):\n",
    "\n",
    "$y_{(t)}= \\phi(W_x^T x_{(t)} + W_y^T y_{(t-1)} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Cells\n",
    "\n",
    "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from previous time steps, we can say it has some sort of memory. \n",
    "\n",
    "This part of the NN is called a **memory cell**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Sequences\n",
    "\n",
    "There are several types of input-output sequences:\n",
    "\n",
    "* Sequence-to-sequence (e.g. for stock prices predictions)\n",
    "* Sequence-to-vector = **encoder** (e.g. sentiment score)\n",
    "* Vector-to-sequence = **decoder** (e.g. caption for image)\n",
    "\n",
    "We can also combine them. A typical example is using encoders-decoders back to back for machine translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNNs\n",
    "\n",
    "The trick is to _unroll it through time_ and then use backprop. This is called **backprop through time** (BPTT).\n",
    "\n",
    "Simply put, we have:\n",
    "\n",
    "1. First pass through unrolled network\n",
    "2. Output sequence evaluated using a cost function\n",
    "3. Gradients of that cost function are then propagated backward through the unrolled network\n",
    "4. Model parameters are updated using the gradients computed during BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting a Time Series\n",
    "\n",
    "There are two classifications of time series based on variables: **univariate** and **multivariate**.  \n",
    "Two more based on our goal: **forecasting** or **imputation** (missing past values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually time series are 3D arrays [batch size, time steps, dimensionality]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Metrics\n",
    "\n",
    "The simplest approach is to predict the last value in each series (**naive forecasting**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020701446"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras \n",
    "\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another simple approach is to use a fully connected flattened network. In our example below, we will use LR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[50, 1]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Simple RNN\n",
    "\n",
    "Now let's try to beat our naive metrics! Here is the simplest possible RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep RNNs\n",
    "\n",
    "Let's add more layers of cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "keras.layers.SimpleRNN(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this works well (better than our LR model), it might be preferable to replace the output layer with a `Dense` layer: it would run slightly faster, the accuracy would be roughly the same, and it would allow us to choose any output activation function we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "keras.layers.SimpleRNN(20),\n",
    "keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting Several Time Steps Ahead\n",
    "\n",
    "But what if we want to go further than one step ahead? As intuition would suggest, the key idea is to use the to-be-predicted value(s) as input(s) for the next one(s).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(1, n_steps + 10)\n",
    "\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "    \n",
    "Y_pred = X[:, n_steps:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful metric here could be comparing it with a naive prediction (i.e. constant values) or a simple linear model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second option is to train an RNN to predict all 10 next values at once (sequence-to-vector, 10 values instead of 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need the output layer to have 10 units instead of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "keras.layers.SimpleRNN(20),\n",
    "keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works very well (better than the linear model) but we can push things further. We can train our model to forecast he next 10 values at each and every time step (sequence-to-sequence). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\n",
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: instead of a one-shot prediction we could adopt techniques such as Montecarlo dropout to understand how are predictions behave in multiple runs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Long Sequences\n",
    "\n",
    "An unrolled RNN on long sequences is a deep network and suffers from the same issues. Let's start to tackle the unstable gradient problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fighting the Unstable Gradients Problem\n",
    "\n",
    "Out of the tricks we already know, nonsaturating activation functions (e.g. ReLU) may not work well. Moreover, Batch Normalization does not work particularly well either, except for small benefits when applied between layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another form of normalization often works better with RNNs: **Layer Normalization**. Similar to BN, it normalizes across the features dimension instead of the batch dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.state_size = units\n",
    "        self.output_size = units\n",
    "        self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
    "        activation=None)\n",
    "        self.layer_norm = keras.layers.LayerNormalization()\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "        norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "        return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tackling the Short-Term Memory Problem\n",
    "\n",
    "After a while, RRNs _forget_ initial inputs. To tackle this problem, various types of cells with long-term memory have been introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM cells\n",
    "\n",
    "Long Short-Term Memory (LSTM) cell can be easily implemented as a black box in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But how does it work? Similarly to a normal cell, except that its state is split into two vectors: **$h_{(c)}$** (short-term state) and **$c_{(t)}$** (long-term state). \n",
    "\n",
    "The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](images/15.LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very short explanation. Starting from the four connected layers:\n",
    "\n",
    "1. f(t) decides how much of $c_{(t-1)}$ is \"**[f]**orgotten\" (dropped)\n",
    "2. g(t) is similar to our basic cell, which only has this layer\n",
    "3. i(t) decides how much of **[i]**nputs after g(t) should be added to long-term state\n",
    "4. o(t) controls which parts of the long-term state should be read and **[o]**utput at this time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, an LSTM cell can:\n",
    "* Learn to recognize an important input (that’s the role of the input gate)\n",
    "* Store it in the long-term state\n",
    "* Preserve it for as long as it is needed (that’s the role of the forget gate)\n",
    "* Extract it whenever it is needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
