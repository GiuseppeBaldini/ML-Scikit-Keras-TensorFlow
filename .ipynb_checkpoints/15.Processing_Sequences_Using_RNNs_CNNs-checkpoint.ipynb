{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Processing Sequences Using RNNs and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Chapter we will cover Recurrent Neural Networks, especially useful with time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neurons and Layers\n",
    "\n",
    "A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](images/15.RNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the weight vectors for inputs $w_x$ and the ones for outputs $w_y$. We can put all these vectors in two matrices $W_x$ and $W_y$. \n",
    "\n",
    "The output vector for the layer would therefore be ($b$ = bias vector; $\\phi$ = activation function):\n",
    "\n",
    "$y_{(t)}= \\phi(W_x^T x_{(t)} + W_y^T y_{(t-1)} + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory Cells\n",
    "\n",
    "Since the output of a recurrent neuron at time step $t$ is a function of all the inputs from previous time steps, we can say it has some sort of memory. \n",
    "\n",
    "This part of the NN is called a **memory cell**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input and Output Sequences\n",
    "\n",
    "There are several types of input-output sequences:\n",
    "\n",
    "* Sequence-to-sequence (e.g. for stock prices predictions)\n",
    "* Sequence-to-vector = **encoder** (e.g. sentiment score)\n",
    "* Vector-to-sequence = **decoder** (e.g. caption for image)\n",
    "\n",
    "We can also combine them. A typical example is using encoders-decoders back to back for machine translation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNNs\n",
    "\n",
    "The trick is to _unroll it through time_ and then use backprop. This is called **backprop through time** (BPTT).\n",
    "\n",
    "Simply put, we have:\n",
    "\n",
    "1. First pass through unrolled network\n",
    "2. Output sequence evaluated using a cost function\n",
    "3. Gradients of that cost function are then propagated backward through the unrolled network\n",
    "4. Model parameters are updated using the gradients computed during BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting a Time Series\n",
    "\n",
    "There are two classifications of time series based on variables: **univariate** and **multivariate**.  \n",
    "Two more based on our goal: **forecasting** or **imputation** (missing past values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
