{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANN) is a Machine Learning model inspired by the networks of biological neurons found in our brains.  \n",
    "\n",
    "Although they draw from our biological brains, they have slightly evolved to be somehow different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it different this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNs have been around for quite some time, with efforts going back to a seminal paper by McCulloch and Pitts (1943). However, after a long winter started in the 1960s they seem to be back in town as the cool kid. \n",
    "\n",
    "Why would this time be different? According to the author:\n",
    "\n",
    "**1. Data quantity**, which allows ANNs to perform traditional ML on large and complex problems\n",
    "\n",
    "**2. Computing power**, thanks to Moore's Law, the gaming industry (for GPUs) and cloud computing \n",
    "\n",
    "**3. Improved algorithms** (not that different from 1990s, but those differences had huge impact)\n",
    "\n",
    "**4. Theoretical limitations** (e.g. getting stuck in local optima) are **rather rare** in practice, or **not as serious** as previously thought\n",
    "\n",
    "**5. Virtual cycle** of applications > reaserch + funding > more and better applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Computations with Neurons\n",
    "\n",
    "_**Note**: Skipping the part on biological neurons_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McCulloch and Pitts proposed a simple model of the biological neuron later known as an **artificial neuron** characterized by one or more binary inputs and one binary output. \n",
    "\n",
    "Even with this simple neuron, it is possible to build an ANN that computes any logical proposition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ANN](images/10.ANN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an activation threshold of two, we can see from the picture above how this could work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in complexity is the _Perceptron_. Invented in 1957 by Frank Rosenblatt, it is based on an artificial neuron called a _threshold logic unit_ (TLU) / _linear threshold unit_ (LTU). Inputs and outputs are numbers and each input is connected with a weight.\n",
    "\n",
    "TLU computes a weighted sum and then applies a step function to the sum:\n",
    "\n",
    "1. $z = w_1x_1 + w_2n_2 + \\cdots + w_nx_n = x^tw$\n",
    "\n",
    "2. $h_w(x) = step(z) = step(x^tw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Threshold Logic Unit](images/10.TLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two common step functions used in Perceptrons:\n",
    "\n",
    "1. Heaviside (z) $ = \\begin{cases}\n",
    "0 & z < 0\\\\\n",
    "1 & z \\ge 0\n",
    "\\end{cases}$\n",
    "\n",
    "2. Sign (z) $ = \\begin{cases}\n",
    "-1 & z < 0 \\\\\n",
    "0 & z = 0 \\\\\n",
    "+1 & z > 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single TLU can be used for linear binary classification (using a threshold, similarly to LogReg or SVM).  \n",
    "A Perceptron is simply a single layer of TLUs, with each TLU connected to all the inputs. Having multiple output TLU makes possible to perform multioutput classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then possible to compute the outputs of a layer of artificial neurons for several instances at once:\n",
    "\n",
    "$h_{W,b} (X) = \\phi (XW + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ = matrix of input features  \n",
    "\n",
    "$W$ = weights of neurons (expept bias). One row per input neuron and one col for artificial neuron. \n",
    "\n",
    "$b$ = vector containing all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.\n",
    "\n",
    "$\\phi$ = activation function. In our case (artificial neuron = TLU), the activation function is a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "The Perceptron is then trained reinforcing connections that help **reduce the error**. \n",
    "\n",
    "More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally:\n",
    "\n",
    "$w_{i,y}^{(next step)} = w_{i,y} + \\eta (y_j - \\hat{y}_j)x_i $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w_{i,y}$ = connection weight between $i^{th}$ input neuron and $j^{th}$ output neuron\n",
    "\n",
    "* $x_i$ is the $i^{th}$ input value of the current training instance\n",
    "\n",
    "* $\\hat{y}_j$ is the output of the $j^{th}$ output neuron for the current training instance\n",
    "\n",
    "* $y_j$ is the target output of the $j^{th}$ output neuron for the current training instance\n",
    "\n",
    "* $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the Perceptron decision boundaries are linear, but as long as training instances are linearly separable, it will converge to a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron is a fairly rudimentary ANN architecture, incapable for example to solve the exclusive or (XOR) classification problem. \n",
    "\n",
    "It turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons: the resulting ANN is known as **Multilayer Perceptron (MLP)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron and Backpropagation\n",
    "\n",
    "An MLP is composed of multiple layers, generally named *input* (lower) layers, *hidden* layers, *output* (upper) layers. \n",
    "\n",
    "**Note**: so far we are only dealing with feedforward neural network (FNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation comes into play as a training method for MLP. In short, it computes the the gradient of the network’s error with regard to every single model parameter by going forward and then backwards. It does so many times until it converges. \n",
    "\n",
    "**Note**: More specifically, automatically computing gradients is called automatic differentiation, or **autodiff**. The autodiff technique used by backpropagation is called **reverse-mode autodiff**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run through the algorithm:\n",
    "\n",
    "1. **Forward pass**: each mini-batch of training instances is passed to the network’s input layer > hidden layers > output layers\n",
    "\n",
    "2. **Error measurement**: network error computed using a loss function that compares the desired output and the actual output of the network\n",
    "\n",
    "3. **Error attribution**: it computes how much each output connection contributed to the error\n",
    "\n",
    "4. **Reverse pass**: tracing back error from output layer to lower layer down to inputs\n",
    "\n",
    "5. **Gradient Descent**: tweak all the connection weights in the network, using the error gradients computed earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: it is important to randomize the initial hiddent layers weights (_break the simmetry_). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this to work, David Rumelhart, Geoffrey Hinton, and Ronald Williams replaced the step function with the logistic (sigmoid) function: \n",
    "\n",
    "$\\displaystyle \\sigma(z) = \\frac{1}{1+ e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was particularly helpful for Gradient Descent, since with a stepwise function there is no gradient to work with. Secondly, having a non-linear function (and a chain of non-linear function over many layers) allows us to theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression MLPs\n",
    "\n",
    "We can use MLPs for single variable regression tasks (with a single output neuron) or multivariable regression (with multiple output neurons). \n",
    "\n",
    "A typical regression MLP architecture consists of the following **hyperparameters**:\n",
    "* n. Input neurons\n",
    "* n. Hidden layers\n",
    "* n. Neurons per hidden layer\n",
    "* n. Output neurons\n",
    "* Hidden activation function\n",
    "* Output activation function\n",
    "* Loss activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification MLPs\n",
    "\n",
    "For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. \n",
    "\n",
    "MLP classification **hyperparameters**:\n",
    "* n. Input neurons\n",
    "* n. Hidden layers\n",
    "* n. Output neurons\n",
    "* Output layer activation (log / softmax)\n",
    "* Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing MLPs with Keras\n",
    "\n",
    "[Keras](https://github.com/keras-team/keras) is a high-level Deep Learning API. On the backend, it relies on one of three popular open source Deep Learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano.\n",
    "\n",
    "**Note**: for simplicity we will use the Tensorflow implementation `tf.keras` (without any specific TF features) therefore altough the code in this chapter can be used on any backend implementation (_usually by changing the imports_).\n",
    "\n",
    "First thing, let's make sure that [Tensorflow](https://www.tensorflow.org/) is up and running (installing it in a virtualenv is recommended):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-17-65996cac02ec>\", line 1, in <module>\n",
      "    tf.__version__\n",
      "AttributeError: module 'tensorflow' has no attribute '__version__'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'AttributeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "  File \"C:\\Users\\giuse\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 953, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
