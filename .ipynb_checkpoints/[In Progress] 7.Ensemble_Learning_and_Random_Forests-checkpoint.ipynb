{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the result of different predictors is called **Ensemble Learning**. Similarly to the phenomenon known as _wisdom of the crowd_, these aggregated models often turn out to be more effective than any individual one. \n",
    "\n",
    "**Note**: Ensemble methods work best when the predictors are as independent from one another as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes (**hard voting**) or predict the class with the highest class probability, averaged over all the individual classifiers (**soft voting**) assuming all the classifiers outputs are probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "The method we discussed above involves using different classifiers on the same dataset. Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set. \n",
    "\n",
    "This second method has two variants:\n",
    "1. **Bagging** (with replacement)\n",
    "2. **Pasting** (without replacement)\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. Usually this means around 37% of instances never sampled (they are left **out-of-bag** so to speak). We can therefore use them to evaluate our predictors, since our predictor has never seen them in training.  \n",
    "\n",
    "Simply add `oob_score=True` to the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Patches and Random Subspaces\n",
    "\n",
    "Sometimes, especially when dealing with high dimensionality, it may be helpful to sample a subset of **features** rather than samples. \n",
    "* Sampling both training instances and features is called the **Random Patches** method\n",
    "* Keeping all training instances and sampling features is called the **Random Subspaces** method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "A Random Forest is an ensemble of Decision Trees, generally trained using bagging and with `max_samples` = training set. \n",
    "\n",
    "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. This results in greater diversity > high bias / low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra-Trees\n",
    "\n",
    "It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds. We call this type of tree **Extremely Randomized Trees** (Extra).\n",
    "\n",
    "Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    "\n",
    "Another great property of Random Forest is that they make easy to measure the relative importance of each feature. \n",
    "This is very easy to see with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09600708420887698\n",
      "sepal width (cm) 0.023022717399856896\n",
      "petal length (cm) 0.43234024077555183\n",
      "petal width (cm) 0.4486299576157143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, Random Forests are very useful if we need a quick understanding of our features, potentially to perform further **feature selection**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "\n",
    "The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. \n",
    "\n",
    "We will cover two of them: Adaptative Boosting (**AdaBoost**) and **Gradient Boosting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### AdaBoost\n",
    "\n",
    "Adaptative boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
