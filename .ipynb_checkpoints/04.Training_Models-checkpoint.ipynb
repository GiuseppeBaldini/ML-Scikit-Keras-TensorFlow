{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating algorithms as black boxes is definitely faster and it does the trick, but understand what is under the hood is essential to really have a firm grasp on our systems and being able to adapt them to what we need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will :\n",
    "\n",
    "1. Work on a Linear Regression model, training it using both a **closed-form** equation and **Gradient Descent**\n",
    "2. Introduce **Polynomial regression**, useful for non-linear datasets but more prone to overfitting (regularization will be needed)\n",
    "3. We will look at two more models commonly used for classification: **Logistic Regression** and **Softmax Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "Generally, a linear model makes a prediction by simply computing a weighted sum of the **input features**, plus a constant called the **bias term** (also called the _intercept term_).\n",
    "\n",
    "$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in vectorized form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = h_{\\theta}(x) = \\theta \\cdot x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model, what we want to do is **minimize the Mean Square Error (MSE)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{MSE }(X, h_\\theta) = \\frac{1}{m} \\sum^{m}_{i = 1}(\\theta^T x^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation\n",
    "\n",
    "It is simply the _closed-form solution_ (an equation that computes the result directly without intermediate steps) for our minimization problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta_{min} = (X^T X)^{-1} X^T y$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.07226838],\n",
       "       [3.06971898]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.07226838],\n",
       "       [10.21170633]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making predictions using theta_best\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.07226838]), array([[3.06971898]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.07226838],\n",
       "       [10.21170633]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a very generic optimization algorithm capable of iteratively finding optimal solutions to a wide range of problems. \n",
    "\n",
    "Given its iterative approach, GD may run into issues when it comes to local/global minima. Luckily for us, the Linear Regression cost function is _convex_ (it only has one global minimum).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: when using Gradient Descent, we should ensure that all features have a similar scale for faster convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the most common implementation, going over the whole training set at each GD step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta_{\\text{(next step)}} = \\theta - \\eta \\nabla_{\\theta} \\text{MSE}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.07226838],\n",
       "       [3.06971898]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as our Normal Equation result above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Obviously, Batch Gradient Descent can get very slow for big datasets, since it is working on the whole training set. On the opposite of the spectrum, we have Stochastic GD, using a random instance in the training set at every step. \n",
    "\n",
    "Due to its stochastic nature, it has a better chance to find global optima for functions which are not strictly convex, but it is also not bound to settle for any specific minimum, but rather staying in a \"good enough\" area.\n",
    "\n",
    "A solution to this dilemma may be to gradually decrease the learning rate (according to a **learning schedule**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index + 1]\n",
    "        yi = y[random_index:random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.06718204],\n",
       "       [3.04745103]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When using Stochastic Gradient Descent, the training instances must be independent and identically distributed (IID), to ensure that the parameters get pulled towards the global optimum, on average. A simple way to ensure this is to **shuffle the instances during training**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.09342338]), array([3.11721836]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor # Stochastic Gradient Descent Regressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch Gradient Descent \n",
    "\n",
    "The middle way. Instead of going full-training-set (Batch) or one-instance-only (Stochastic) Mini-batch GD computes the gradients on small random sets of instances called _... drum rolls..._ mini-batches.\n",
    "\n",
    "To conclude, here is a summary table for the different algorithms available for Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison Table of algorithms for Linear Regression](images/4.Lin_Reg_algos_comparison.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
