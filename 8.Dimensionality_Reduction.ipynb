{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enourmous scale of data available is a double-edged sword. Massive dataset can be slow to process, leading to what is usually called the **curse of dimensionality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real work applications, we can the methods used to reduce this size to a treatable scale **dimensionality reduction**. \n",
    "Clear advantages are: \n",
    "1. Lower memory requirements (obviously)\n",
    "2. Faster computation \n",
    "3. Ease of visualization, if we manage to get down to 2D/3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, there are two main approaches to dimensionality reduction: projection and manifold learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection\n",
    "\n",
    "The idea behind projection is fairly simple: in most cases, instances are not evenly spread out, but highly correlated. As a result, all training instances actually lie within\n",
    "(or close to) a much lower-dimensional _subspace_ of the high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold Learning\n",
    "\n",
    "A _d-dimensional_ manifold is a part of an _n-dimensional_ space (where d < n) that locally resembles a _d-dimensional_ hyperplane. \n",
    "\n",
    "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called **Manifold Learning**. This works due to two main hypotheses:\n",
    "\n",
    "1. Most real-world high-dimensional datasets lie close to a much lower-dimensional manifold, an hypothesis that holds true empirically very often. \n",
    "2. Task will be easier to perform on lower-dimensional space. This does not always hold true so we should be cautious to understand if this is the case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is by far the most popular dimensionality reduction algorithm. It works by identifying the hyperplane closest to the data and projecting the data onto it.\n",
    "\n",
    "The hyperplane itself is chosen as the one that **preserve the most variance** as it will most likely retain the most information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal components\n",
    "\n",
    "After idenfying the first axis, we then proceed to find a second one that retains the most of the remaining variance.\n",
    "\n",
    "The unit vector that defines the $i^{th}$ axis is called the $i^{th}$ principal component. \n",
    "\n",
    "We can find PCs of a training set by using a standard matrix factorization technique known as **Singular Value Decomposition** (SVD) that decomposes the training matrix in three values: u, $\\sigma$ and $V^T$ where $V$ contains all the PCs.\n",
    "\n",
    "Using Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, Vt = np.linalg.svd(X_centered)\n",
    "c1 = Vt.T[:, 0]\n",
    "c2 = Vt.T[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: PSA assumes that the dataset is centered. Usually Scikit-learn takes care of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have identified all the principal components, you can reduce the dimensionality of the dataset down to $d$ dimensions by projecting it onto the hyperplane defined by the first $d$ principal components.\n",
    "\n",
    "In Python, we simply have to compute the matrix multiplication of the training matrix $X$ and the matrix containing the first $d$ PCs: \n",
    "\n",
    "$X_{d-proj} = X W_d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = Vt.T[:, :2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explained Variance Ratio\n",
    "\n",
    "EVR indicates the proportion of dataset's variance that lies along the axis of each principal component. This can be a good proxy to understand how much information we are retaining.  \n",
    "\n",
    "This concept is strictly related to how to choose the number of dimensions. In most cases, we start with a target variance and choose the minimum number of dimensions to preserve that variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomized PCA\n",
    "\n",
    "To make things even faster, we can rely on randomized PCA, which works using an approximation of the first $d$ principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incremental PCA\n",
    "\n",
    "Differently from Randomized PCA, Incremental PCA does not require to store the whole training set in memory, making it suitable for batches and online learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
