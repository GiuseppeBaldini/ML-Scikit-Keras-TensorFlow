{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enourmous scale of data available is a double-edged sword. Massive dataset can be slow to process, leading to what is usually called the **curse of dimensionality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real work applications, we can the methods used to reduce this size to a treatable scale **dimensionality reduction**. \n",
    "Clear advantages are: \n",
    "1. Lower memory requirements (obviously)\n",
    "2. Faster computation \n",
    "3. Ease of visualization, if we manage to get down to 2D/3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, there are two main approaches to dimensionality reduction: projection and manifold learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Projection\n",
    "\n",
    "The idea behind projection is fairly simple: in most cases, instances are not evenly spread out, but highly correlated. As a result, all training instances actually lie within\n",
    "(or close to) a much lower-dimensional _subspace_ of the high-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manifold Learning\n",
    "\n",
    "A _d-dimensional_ manifold is a part of an _n-dimensional_ space (where d < n) that locally resembles a _d-dimensional_ hyperplane. \n",
    "\n",
    "Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called **Manifold Learning**. This works due to two main hypotheses:\n",
    "\n",
    "1. Most real-world high-dimensional datasets lie close to a much lower-dimensional manifold, an hypothesis that holds true empirically very often. \n",
    "2. Task will be easier to perform on lower-dimensional space. This does not always hold true so we should be cautious to understand if this is the case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
