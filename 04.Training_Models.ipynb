{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating algorithms as black boxes is definitely faster and it does the trick, but understand what is under the hood is essential to really have a firm grasp on our systems and being able to adapt them to what we need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will :\n",
    "\n",
    "1. Work on a Linear Regression model, training it using both a **closed-form** equation and **Gradient Descent**\n",
    "2. Introduce **Polynomial regression**, useful for non-linear datasets but more prone to overfitting (regularization will be needed)\n",
    "3. We will look at two more models commonly used for classification: **Logistic Regression** and **Softmax Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "Generally, a linear model makes a prediction by simply computing a weighted sum of the **input features**, plus a constant called the **bias term** (also called the _intercept term_).\n",
    "\n",
    "$y = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in vectorized form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = h_{\\theta}(x) = \\theta \\cdot x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train this model, what we want to do is **minimize the Mean Square Error (MSE)**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{MSE }(X, h_\\theta) = \\frac{1}{m} \\sum^{m}_{i = 1}(\\theta^T x^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation\n",
    "\n",
    "It is simply the _closed-form solution_ (an equation that computes the result directly without intermediate steps) for our minimization problem: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta_{min} = (X^T X)^{-1} X^T y$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8239648 ],\n",
       "       [3.14613997]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.8239648 ],\n",
       "       [10.11624473]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making predictions using theta_best\n",
    "\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.8239648]), array([[3.14613997]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.8239648 ],\n",
       "       [10.11624473]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent is a very generic optimization algorithm capable of iteratively finding optimal solutions to a wide range of problems. \n",
    "\n",
    "Given its iterative approach, GD may run into issues when it comes to local/global minima. Luckily for us, the Linear Regression cost function is _convex_ (it only has one global minimum).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: when using Gradient Descent, we should ensure that all features have a similar scale for faster convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's the most common implementation, going over the whole training set at each GD step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\theta_{\\text{(next step)}} = \\theta - \\eta \\nabla_{\\theta} \\text{MSE}(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.8239648 ],\n",
       "       [3.14613997]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as our Normal Equation result above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "\n",
    "Obviously, Batch Gradient Descent can get very slow for big datasets, since it is working on the whole training set. On the opposite of the spectrum, we have Stochastic GD, using a random instance in the training set at every step. \n",
    "\n",
    "Due to its stochastic nature, it has a better chance to find global optima for functions which are not strictly convex, but it is also not bound to settle for any specific minimum, but rather staying in a \"good enough\" area.\n",
    "\n",
    "A solution to this dilemma may be to gradually decrease the learning rate (according to a **learning schedule**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index + 1]\n",
    "        yi = y[random_index:random_index + 1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.80709325],\n",
       "       [3.14798832]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When using Stochastic Gradient Descent, the training instances must be independent and identically distributed (IID), to ensure that the parameters get pulled towards the global optimum, on average. A simple way to ensure this is to **shuffle the instances during training**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.79474325]), array([3.13569893]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor # Stochastic Gradient Descent Regressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini-batch Gradient Descent \n",
    "\n",
    "The middle way. Instead of going full-training-set (Batch) or one-instance-only (Stochastic) Mini-batch GD computes the gradients on small random sets of instances called _... drum rolls..._ mini-batches.\n",
    "\n",
    "To conclude, here is a summary table for the different algorithms available for Linear Regression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Comparison Table of algorithms for Linear Regression](images/4.Lin_Reg_algos_comparison.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "We can also use linear models to fit nonlinear data by adding powers of each feature as new features, then train a linear model on this extended set of features. This technique is called **Polynomial Regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating non-linear (quadratic) data\n",
    "\n",
    "m = 100\n",
    "X = 6 * np.random.rand(m, 1) - 3\n",
    "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use Scikit-Learn’s `PolynomialFeatures` class to transform our training data, adding the square of each feature in the training set as new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20044472])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False) # excluding bias term\n",
    "X_poly = poly_features.fit_transform(X)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20044472, 0.04017809])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_poly[0] # original feature + squared feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit a Linear Regression to this extended training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.14296606]), array([[0.94808845, 0.46664261]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_poly, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Given **n** features, `PolynomialFeatures(degree=d)` returns an array of $\\displaystyle \\frac{(n + d)!}{d!n!}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "A standard way to understand if our model is under/overfitting the data is to look at the learning curves: plots of the model’s performance on the training set and the validation set as a function of the training set size (or the training iteration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking:\n",
    "\n",
    "1. Converging + high error = **underfitting**\n",
    "2. Not converging + lower error = **overfitting** (adding more data can help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: A model’s generalization error can be expressed as the sum of three different errors:\n",
    "\n",
    "1. **Bias**: due to wrong assumptions > likely to underfitting\n",
    "\n",
    "2. **Variance**: due to excessive sensitivity to small variations in the training data > likely to overfit\n",
    "\n",
    "3. **Irreducible error**: due to the intrinsic noisiness of the data > we can reduce it by cleaning up the data (broken sources, outliers, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Models\n",
    "\n",
    "For a linear model, regularization is typically achieved by constraining the weights of the model.\n",
    "\n",
    "Let's look at three different ways on how this can be achieved："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge Regression\n",
    "\n",
    "A regularization term $\\alpha \\sum_{i = 1}^n \\theta^2_i$ is added to the cost function we are trying to minimize: $J(\\theta) = MSE(\\theta) + \\alpha\\frac{1}{2}\\sum^n_{i=1} \\theta^2_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1**: the regularization term should only be added to the cost function during training.  \n",
    "**Note 2**: Remember to scale the data before performing regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression\n",
    "\n",
    "Shorter for _Least Absolute Shrinkage and Selection Operator Regression_. It uses the $l_1$ norm of the weight vector instead of half the square of the $l_2$ norm: $J(\\theta) = MSE(\\theta) + \\alpha\\sum^n_{i=1} | \\theta_i |$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero).  \n",
    "In other words, Lasso Regression automatically performs feature selection and outputs a **sparse model** (i.e., with few nonzero feature weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elastic Net\n",
    "\n",
    "It's a mix of Ridge and Lasso, which we can control with a parameter $r$: \n",
    "* r = 0 > Ridge Regression\n",
    "* r = 1 > Lasso Regression\n",
    "\n",
    "$J(\\theta) = MSE(\\theta) + r\\alpha\\sum^n_{i=1} |\\theta_i| + \\frac{1-r}{2} \\alpha \\sum^n_{i=1} \\theta^2_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to choose a Linear Model\n",
    "\n",
    "Two general tips:\n",
    "\n",
    "1. **Ridge** is a good default, but if you suspect that only a few features are actually useful, you should prefer Lasso or Elastic Net\n",
    "\n",
    "2. **Elastic Net** is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Early stopping\n",
    "\n",
    "We can also decide to stop GD as soon as it reaches a certain error level. This may be particularly helpful in complex models where the **validation error** stops decreasing and actually starts to go back up.  \n",
    "\n",
    "For Stochastic or Mini-Batch GD, since they are not _smooth_, we may want to stop only after the error has been under the minimimum for a certain number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (also called _Logit Regression_) is commonly used to estimate the probability that an instance belongs to a particular class.  \n",
    "If the probability is above a certain **threshold**, the model predicts that the instance belongs to that class making Logistic Regression a **binary classifier**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression model does, it outputs the **logistic** of this result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p = h_{\\theta}(x) = \\sigma(x^T \\theta)$\n",
    "\n",
    "Where $\\sigma$ is a sigmoid function which outputs a number between 0 and 1:\n",
    "\n",
    "$\\displaystyle \\sigma(t) = \\frac{1}{1+ exp(-t)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our _log loss_:\n",
    "\n",
    "$J(\\theta) = -\\frac{1}{m} \\sum^m_{i=1} [y^{(i)} log(p^{(i)}) + (1-y^{(i)}) log (1-p^{(i)})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Boundaries\n",
    "\n",
    "Let's build a classifier to detect the Iris-Virginica using a well-known dataset and Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:] # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training our Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "log_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xf5ea4f0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XucTfX+x/HXZ+4uw2DIdYxCCHEalw7lkk7Uafg5dSiRUx1dkO6HnCOXEJKjUFEOOkWSREdE6SKRUSiUxiWN67gNhrl/f398hxnTmNnYe9a+fJ6Px3rstfdae6/Paus9a3/XWt+vGGNQSinlX4KcLkAppZT7abgrpZQf0nBXSik/pOGulFJ+SMNdKaX8kIa7Ukr5IQ13pZTyQxruSinlhzTclVLKD4U4teHo6GgTGxvr1OaVUsonbdiw4bAxpnJx6zkW7rGxsSQkJDi1eaWU8kki8qsr62mzjFJK+SENd6WU8kMa7kop5Yc03JVSyg8VG+4iMlNEDonIjxdYLiLysogkishmEfmD+8tUSil1MVw5cp8FdC5ieRegXu7UD3j18stSSil1OYoNd2PMl8DRIlbpCswx1logSkSquatApZRSF88d17nXAH7L9zwp97X9bvjsQr39NiQmQlAQBAfbqXJluO8+u3zRIjhw4PfLb7vNLl+5Ek6cyFsWFGSXt2hhl2/YANnZEBZmp/BwKFfOrgNw5gyEhtr3inhqL5VS/iTH5JCRnUF6VjrhIeFEhER4dHvuCPfC4q3QgVlFpB+26YaYmJhL3uDbb8PHH5//WoMGeeE+cSKsXn3+8ri4vHB/6inYtOn85R06wGef2fkePWDHjvOXx8fDhx/a+Tp14OBBG+zh4fYPwF13wWuv2eWtWtnHMmXsVLYsdO4M994LxsDYsXnLIiOhQgWoXx9iY+3ynBz7h0MpVfKMMaSkp3A87Tgn0k8UOZ1MP8nprNOcyTzDmawzRT6mZ6ef28Zrt73Gg3EPenQ/3BHuSUCtfM9rAvsKW9EYMx2YDhAXF3fJI3MvXWpDMDs7b8o/zvfixZCenrcsJwdC8u3pggWQmnr+8rJl85bPmmWP7NPTISPDTtWr5y1/9llISclblp4O112Xtzwmxr4/NRWOHbOP9evbZWlpMHTo7/dpyBAYMwaOHoXoaChf3oZ+hQpQqRI89BD85S9w6pTdv6pV7VStGkRF6S8IpYqSnZPNwdSDJJ1IYu+JvSSdSOJQ6iEOnz5M8ulkDp8+fG46cuYIWTlZRX6eIJQLL0fZsLKUCi1FqZBS5x6jS0ef9zz/fERIBOEh4Vxf63qP77M7wn0xMEBE5gGtgBRjjMeaZM4SsYEdUsgeVKhQ9Hvr1i16edu2RS9/9NGil7/33oWXRUTYgE9NtdPJk/YPQNWqdnlICDz3nH3t2DEb9keO2PcA7NoFvXqd/5llysCMGfbXw9698O679tdFnTpQr55drpQ/M8Zw4NQBEo8m5k3HEtmTsoekE0nsP7mfbJN93nuCJIhKpSoRXTqa6NLR1K9Unza12hBdOppKpStRsVRFyoWXK3QqHVqaIPHuK8mLDXcRmQu0B6JFJAl4DggFMMa8BiwFbgUSgdPA3zxVrD8425QTHg4VK/5+efnyMHz4hd9/9dWwbZs9p7B/v51++y3vl8HGjfDkk+dvLzYW/vtf+OMf7fr79kHjxrYGpXzNwVMH2Xxws50O2cdfjvxCambquXWCJZg6FeoQGxXLTXVuoma5mtSIrGEfy9WgRmQNoktHExzkv+2fxYa7MeauYpYboL/bKlJFCguz5xcaNCh8+a232iP93bvteYOffoItW/J+GSxcCAMG2BPC115rTyK3aAF33nl+05RS3uBE+gm+3fsta5PW8k3SNyTsS+BQ6qFzy6uVrUaTK5rQrnY76lasS72K9ahbsS4x5WMIDQ51sHLniTGX3PR9WeLi4oz2Clny9u2DNWsgIQHWr7ePJ0/acwiRkfZ8xP799gTzNddoW74qWSfST/DF7i9YuXMlq3av4sdDP2Jyr89oVLkRLWu0pNkVzWhyRROaVGlC5TLF9nzrd0RkgzEmrrj1HOvyVzmjenW44w47gT2ZvHu3DXawR/Zz59r5KlXsL4Hu3eH22x0pV/k5Yww/HvqRRT8tYvmO5axNWku2yaZUSCluqH0DdzS6g9Y1W9OyRkuiIqKcLten6JG7+p3du2HVKns/wNKl9jLSFSvssiVL4MYb7bkBpS6FMYZ1e9excNtCPvjpAxKPJiIIcdXjuPnKm7n5qpu5vub1hIfoSaHCuHrkruGuipSZCcnJ9oj/8GG44grbXh8fD7172+v3QwO7aVO5aE/KHuZsmsPsTbNJPJpIaFAoHet0pHvD7sRfHU/VslWdLtEnaLOMcovQ0Lxr/CtVgrVr4a23bNPNe+/Zu3bnzoWbbnK2TuWdsnOyWbJ9CVPXT+XTnZ9iMLSPbc/QG4bSrUE3bWrxIA135TKRvKtrJk6E5cvhzTehYUO7fN06e6Rf3H0Cyv+lpKXw5vdvMuXbKew6vouY8jEMbz+c3k17U6dCHafLCwga7uqShIbCn/9sp7PGjLF3z954IwwbBh076tU2gebomaO89M1LvLzuZU5mnKRtTFsm3DyBrg26EhKkcVOS9L+2cpu5c+GNN2DcOOjUyd40NWYMtGvndGXK046dOcZL37zE5HWTOZVxijsa3cHgtoP5QzUd3sEp3n3/rPIppUvbrhl27ICpU+2ds99/73RVypMyszN5Zd0rXPXyVTz/1fN0rtuZzQ9vZv6d8zXYHaZH7srtIiLgkUdsL51BuYcP//2vPRk7cmTh3S4o37MscRlPLH+CbYe30enKTkz800SaXtHU6bJULj1yVx4TEWG7SwDb//5rr9mTr/Pnn9+Lp/ItB04d4M737qTL213IzMnkw54f8sk9n2iwexkNd1Uihg+33R3UqmX7y4+Ph6Qkp6tSF8MYw5xNc2g0tRFLfl7C6I6j+fHhH4m/Oh7RM+deR8NdlZjmzW3TzIsv2oFR1q1zuiLlquTUZOLnxXPvontpWLkhGx/ayLM3PKt3kXoxDXdVokJCbJfEu3fbwUfAdm1w6pSjZakirNq1imtfu5ZPdnzCpFsm8WXfL2kQfYFuSZXX0HBXjjg7Hu2hQ9C1qx3Jats2Z2tS58vOyeZfn/2Lm+bcRLnwcqx7YB2PtX7Mr/tA9yca7spRVarYzslSUuzYs2fHqVXOSklLIX5ePM9/9Tz3NruXDf020KxqM6fLUhdBw105rn1726/81VdDt272cknlnF+O/ELrN1vzyY5PePW2V/lP1/9QJkzHavQ1ep278go1a8JXX9mBwDMzna4mcK3atYru87sTLMGs7L2SdrF6e7Gv0nBXXiMiAv7zn7xr4H/4AWJitO/4kvL+1ve5e+Hd1KtYjyV3LdEOvnycNssoryJi72o9c8b2FX/jjXbYP+VZ0zdM58737iSuehxf/u1LDXY/oOGuvFKpUvYofscO2ya/d6/TFfmvsV+N5cGPHuTWereyovcKKpbS/iH8gYa78lp/+pPtM37/ftuz5G+/OV2R/xn71Vie/exZejXpxQc9PqB0aGmnS1JuouGuvFqbNjbgk5Ph+eedrsa/TPh6wrlgn91tNqHBOl6iP9ETqsrrXX89fP011K3rdCX+Y9I3k3hm5TP0bNyTWd1m6Y1JfkiP3JVPaNzYXk1z5Aj8/e9w4oTTFfmu/3z/H5745AnuaHQHb/3fWzpCkp/ScFc+ZdMmmDXL9ip55ozT1fiej3/5mL8v+Ts3X3kzb3d/W4Pdj2m4K5/SsSPMng1ffAF9+kBOjtMV+Y6EfQnc+d6dNL2iKe//9X3CgsOcLkl5kIa78jl33w0TJsCCBXYgblW8ncd2cts7txFdOpr/3f0/IsMjnS5JeZiGu/JJTz4JDzwA77wDJ086XY13O5l+ktvn3k5mdibL7llGtchqTpekSoA2uCmfJALTptneJCP1IPSCckwOfRb14afDP7H8nuXaD3sA0SN35bNCQyE6GtLTYfBg7aagMM9/+TyLflrEize/SKcrOzldjipBLoW7iHQWkZ9FJFFEBheyPEZEVonI9yKyWURudX+pShVu1y545RX461+1R8n8PvzpQ577/Dl6N+3NY60fc7ocVcKKDXcRCQamAl2ARsBdItKowGr/BOYbY5oDPYFp7i5UqQtp0ABmzIDVq2HIEKer8Q47j+2kz6I+xFWP4/U/v64DWAcgV47cWwKJxpidxpgMYB7QtcA6BiiXO18e2Oe+EpUq3t13w4ABMHGivYomkGVkZ9BjQQ+CJIgFdy6gVGgpp0tSDnAl3GsA+btsSsp9Lb/hwD0ikgQsBQa6pTqlLsLEidC6NQwcCGlpTlfjnMErB5OwL4E349+kdlRtp8tRDnEl3Av7PWcKPL8LmGWMqQncCrwlIr/7bBHpJyIJIpKQnJx88dUqVYSwMHj3Xfj8c9tVQSD6aPtHTFo7iQEtBtC9YXeny1EOciXck4Ba+Z7X5PfNLvcD8wGMMd8AEUB0wQ8yxkw3xsQZY+IqV658aRUrVYSYGDsWqzF2JKdAsvfEXu5ddC/NqjZjwp8mOF2Ocpgr4b4eqCcidUQkDHvCdHGBdfYANwGISENsuOuhuXLMq69C8+awdq3TlZQMYwwPLHmAtKw03r3jXSJCAvSnizqn2HA3xmQBA4DlwDbsVTFbRGSkiMTnrvYk8HcR2QTMBfoaYwo23ShVYnr1soNu9+oVGHewvvHdGyxLXMb4TuOpX6m+0+UoLyBOZXBcXJxJSEhwZNsqMKxebcdgfeghezerv9p1bBdNX2tKqxqt+KT3JwT9/nSX8iMissEYE1fcevqvQPmttm3h8cdtE83nnztdjWfkmBz6ftgXQZjZdaYGuzpH+5ZRfm3UKNvufvq005V4xivrXuHLX79kZvxMYsrHOF2O8iIa7sqvlS5tm2f88QbNPSl7GPrZULrU7ULfZn2dLkd5Gf0Np/yeCGRl2T7g16xxuhr3MMbQf2l/DIZpt03T7gXU72i4q4Bw5gxMmQL9+vlH52Lvb3ufj7Z/xMj2I4mNinW6HOWFNNxVQIiMtD1HbtkCkyc7Xc3lSUlL4dGPH6V51eYMaj3I6XKUl9JwVwEjPh5uvx2GD4fffit2da815NMhHEw9yPTbp+sA1+qCNNxVQJk82Q6q/fjjTldyab7d+y2vJbzGoy0fJa56sZc6qwCmf/ZVQKlTx97QVK+e05VcvByTw8CPB1K1bFVGdhjpdDnKy2m4q4DTt6/TFVyaOZvm8O3eb5nTbQ6R4TpwrCqaNsuogJSdDf37w7hxTlfimpS0FAavHMz1Na+nV9NeTpejfICGuwpIwcF2QO1Ro3xjYO1RX47iUOohXu7ysnYxoFyi/0pUwBo/HjIyYOhQpysp2k+Hf2Lyusnc3/x+PYmqXKbhrgJW3bowaBDMmgXffed0NRf2+PLHKRNahtE3jXa6FOVDNNxVQPvnP6FSJRg82OlKCrdixwqWJS5jWLthVClTxelylA/Rq2VUQCtfHt5+2zsvjcwxOTyz8hlio2Lp36K/0+UoH6PhrgLen/6UN2+M9/Qg+c4P77DxwEbe6f4O4SHhTpejfIw2yygFnDoFXbrYzsW8QVpWGkM/G8p11a6jR+MeTpejfJCGu1JAmTKQlmYvjfSGMVenfDuFPSl7GH/zeL30UV0S/VejFLYp5oUXIDkZXnrJ2VqOnjnK6K9G06VuFzrW6ehsMcpnabgrlatVK+jeHV580Ya8U0Z/OZqUtBTGdfKR22eVV9JwVyqf0aPteKsvvujM9n89/itT1k+hb7O+NLmiiTNFKL+gV8solU+DBvD++9ChgzPbH/XlKABGtB/hTAHKb2i4K1VAt272saQvi0w8msisjbPo36I/tcrXKrkNK7+kzTJKFWLzZmjeHLZtK7ltjvhiBGHBYQy5YUjJbVT5LQ13pQpRvTrs2AEjSqh1ZGvyVt7e/DYDWg6gatmqJbNR5dc03JUqRHQ0PPoozJ8PP/zg+e0N/3w4ZcLK8EybZzy/MRUQNNyVuoAnn4SyZT1/9L7xwEbe2/oej7V6jOjS0Z7dmAoYGu5KXUDFinYg7ffft23wnjJs1TCiIqJ48o9Pem4jKuDo1TJKFeHxx6F2bWjY0DOf/+3eb1myfQnPd3ieqIgoz2xEBSQNd6WKEBUF993nuc9/7vPnqFSqEo+2etRzG1EByaVmGRHpLCI/i0iiiBQ6rIGI/FVEtorIFhF5x71lKuWsN9+E++9372du2LeBZYnLePL6J4kMj3Tvh6uAV2y4i0gwMBXoAjQC7hKRRgXWqQcMAdoYY64BHvNArUo55uBBmDkTvv3WfZ85ZvUYyoeX55EWj7jvQ5XK5cqRe0sg0Riz0xiTAcwDuhZY5+/AVGPMMQBjzCH3lqmUswYOtCdYR41yz+dtTd7Kwm0LGdhyIOUjyrvnQ5XKx5VwrwH8lu95Uu5r+dUH6ovI1yKyVkQ6F/ZBItJPRBJEJCHZyW73lLpIkZH25OpHH8H331/+541dPZbSoaUZ1HrQ5X+YUoVwJdwL613DFHgeAtQD2gN3AW+IyO9O/Rtjphtj4owxcZUrV77YWpVy1IABUK4cjBlzeZ+z89hO5v4wl4eue0iva1ce48rVMklA/l6MagL7CllnrTEmE9glIj9jw369W6pUygtERcG//w01Cv5uvUjjVo8jOChYr2tXHuXKkft6oJ6I1BGRMKAnsLjAOouADgAiEo1tptnpzkKV8gZ/+9v5A2pfrL0n9jJr0yzua3Yf1SOru68wpQooNtyNMVnAAGA5sA2Yb4zZIiIjRSQ+d7XlwBER2QqsAp42xhzxVNFKOenwYXjqKUhMvPj3vrjmRbJzsrUPGeVxLt3EZIxZCiwt8NqwfPMGeCJ3UsqvZWbClClw/Di88Ybr70tOTeb1Da/Tq2kv6lSo47kClUL7llHqolWrBg88ALNnw549rr/v32v/TVpWGoPbFHofoFJupeGu1CV45hk7StP48a6tn5KWwpT1U+jesDsNK3uooxql8tFwV+oSxMTAvffaZpn9+4tff+r6qZxIP8HQG4Z6vjil0I7DlLpkgwfDqVO2Db4oqRmpTFo7iS51u9C8WvOSKU4FPA13pS7RVVfB3LnFrzfjuxkcPn1Yj9pVidJmGaUu048/woIFhS9Lz0pnwpoJtKvdjjYxbUq2MBXQNNyVukzDh9urZ1JSfr9s9qbZ7Du5T4/aVYnTcFfqMg0ZYoN92rTzX8/KyWLc1+NoUb0Fna7s5ExxKmBpuCt1ma67Djp3hkmT4PTpvNfn/TiPncd2MvSGoYgU1v+eUp6j4a6UGwwdCsnJeXes5pgcxq4eS+Mqjbn96tudLU4FJL1aRik3aNsWbr0178h90U+L2Jq8lXe6v0OQ6DGUKnka7kq5yUcf2btWjTGM+WoMV1W4ijuvudPpslSA0nBXyk1ssMOEtxPYkLSRGd1eIyRI/xdTztDfi0q50aefwj96t6Dizofpc20fp8tRAUwPK5Ryo9CrvoLKFQj7ZjghEuZ0OSqA6ZG7Um409uvRRN40lQM7K7G44HhlSpUgDXel3CRhXwLLdyznHw/GcuWVMHq0bYNXygka7kq5yZivxhAVEcXA6x9m8GBISrKTUk7QcFfKDbYc2sIHP33AwJYDKRdejnvvhV27oFYtpytTgUrDXSk3GLt6LGVCyzCo1SAAwsIgIsL29X7ggMPFqYCk4a7UZdpxdAdzf5zLQ3EPUal0pXOvG2PvXP3b3xwsTgUsDXelLtO4r8cREhTCE9c/cd7rItCtGyxbBhs2OFScClga7kpdhr0n9jJr4yzua3Yf1SOr/275I49A+fIwZowDxamApuGu1GWYsGYCOSaHZ9o8U+jy8uVh4EBYuBC2bi3h4lRA03BX6hIdPHWQ6Rum0/va3tSpUOeC6w0aBKVLw5w5JVicCnja/YBSl2jS2kmkZaUxpO2QIteLjob166FBgxIqTCn0yF2pS3L0zFGmrp9Kj8Y9qF+pfrHrN2oEQUH20kilSoKGu1KX4OV1L3Mq4xTPtn3W5fcsWWJvatq714OFKZVLw12pi3Qi/QST102mW4NuNLmiicvva9wYDh+GiRM9WJxSuTTclbpI09ZP43jacYbeMPSi3lenDtx9N7z+ug15pTzJpXAXkc4i8rOIJIrI4CLWu0NEjIjEua9EpbxHakYqE7+ZSOe6nYmrfvH/zIcMgTNn4N//9kBxSuVTbLiLSDAwFegCNALuEpFGhawXCTwKrHN3kUp5ixnfzeDw6cP884Z/XtL7GzaE7t1hyhRITXVzcUrl48qRe0sg0Riz0xiTAcwDuhay3ihgPJDmxvqU8hppWWlMWDOB9rHtaRPT5pI/5/nn4ZNPoEwZNxanVAGuXOdeA/gt3/MkoFX+FUSkOVDLGPORiDzlxvqU8hqzNs5i38l9zOl2eXcj6fXuqiS4cuQuhbx2bnwZEQkCJgFPFvtBIv1EJEFEEpKTk12vUimHZWRn8MLqF2hdszUd63S87M9LT4f774dp09xQnFKFcCXck4D8Qw7UBPblex4JNAY+F5HdQGtgcWEnVY0x040xccaYuMqVK1961UqVsFkbZ/Fryq/868Z/IVLY8c7FCQ+HxETboVh6uhsKVKoAV8J9PVBPROqISBjQEzg39K8xJsUYE22MiTXGxAJrgXhjTIJHKlaqhKVnpTP6q9G0qtGKLnW7uO1zhw61NzRpnzPKE4oNd2NMFjAAWA5sA+YbY7aIyEgRifd0gUo5beb3M9mTsoeRHUa65aj9rJtvhhYtYOxY7ZZAuZ8Yh4Znj4uLMwkJenCvvFtaVhp1X65LbFQsX/3tK7eGO8DSpXDbbfbGpn793PrRyk+JyAZjTLE3WWivkEoVYcaGGew9uZfZ3Wa7PdgBunSx7e6dO7v9o1WA03BX6gLOZJ5hzOox3Fj7RrdcIVMYEXvXqlLupn3LKHUBr294nQOnDjCyvXvb2guzeTP07AmnT3t0MyqAaLgrVYjUjFTGrh5LxzodaRfbzuPbO3kS3n0Xpk71+KZUgNBwV6oQrya8yqHUQ4xoP6JEttemjW13HzfOBr1Sl0vDXakCUtJSGLt6LH+66k+0jWlbYtsdORKOHIHJk0tsk8qPabgrVcD4r8dz9MxRXrjphRLdbosWEB8PL74Ix46V6KaVH9KrZZTKZ//J/UxaO4mejXvSvFrzEt/+yJGweLHtnkCpy6HhrlQ+I78YSWZOJqM6jHJk+9deayelLpc2yyiV65cjvzDjuxn0+0M/6las62gtixbBsGGOlqB8nIa7Urn+tepfhIeE8692/3K6FNassYN6bN7sdCXKV2m4KwVs2LeBd7e8yxOtn6Bq2apOl8OQIRAVBf/4h9OVKF+l4a4CnjGGp1c8TaVSlXjqj94xkFiFCrZL4GXLYOVKp6tRvkjDXQW8D3/+kFW7VzGyw0jKR5R3upxz+veH2rXhmWcgJ8fpapSv0atlVEBLz0rnqU+eolHlRvS7zrv63I2IsDc0nTjhdCXKF2m4q4A25dsp7Di2g2W9lhES5H3/O3Tt6nQFyldps4wKWMmpyYz8ciS31ruVW+re4nQ5F2QMvPQSvFCyN8wqH6fhrgLWsFXDSM1I5cWbX3S6lCKJwHffwfDhsHOn09UoX6HhrgLS5oObmf7ddB5p8QgNKzd0upxijRsHISHw5JNOV6J8hYa7Cjg5JoeH//cwFUtVZHj74U6X45IaNeCf/7R3rq5Y4XQ1yhdouKuAM3vjbNb8tobxncZTsVRFp8tx2eOPw1VXwWOP6aWRqnjed3mAUh505PQRnl7xNG1qteHeZvc6Xc5FCQ+HN96wzTNBelimiqHhrgLKs58+y/G040y7bRpB4nsJ2b593nx2NgQHO1aK8nK+969bqUu0NmktM76bwaBWg2h6RVOny7ksQ4fC7bfbyySVKoyGuwoImdmZPPy/h6kWWc1nTqIW5Yor4OOPYe5cpytR3krDXQWECWsmsPHARl7p8gqR4ZFOl3PZ+ve3w/I99hgcPep0Ncobabgrv7c1eSsjvhjBnY3upHvD7k6X4xbBwTB9ug32p592uhrljTTclV/Lzsnmvg/vIzIskim3TnG6HLdq1sze1DR3Luzd63Q1yttouCu/NnndZNbtXccrXV6hSpkqTpfjdiNGwMaN9iYnpfLTcFd+6+fDPzP0s6HEXx1Pz8Y9nS7HIyIioH59e9XMmjVOV6O8iYa78ksZ2Rn0WtiL0qGlefW2VxERp0vyqHffhTZt4IMPnK5EeQuXwl1EOovIzyKSKCKDC1n+hIhsFZHNIvKpiNR2f6lKuW7458PZsH8Db9z+BtUjqztdjsd17w7Nm8ODD8KhQ05Xo7xBseEuIsHAVKAL0Ai4S0QaFVjteyDOGNMUWACMd3ehSrnqi91f8MLqF3ig+QP8X8P/c7qcEhEWBnPm2FGb+vbVvmeUa0fuLYFEY8xOY0wGMA84b3wYY8wqY8zp3KdrgZruLVMp1xw7c4zeH/SmbsW6TOo8yelySlTjxjBpkr25aeJEp6tRTnOlb5kawG/5nicBrYpY/37g48IWiEg/oB9ATEyMiyUq5RpjDA8seYD9p/az5r41lA0r63RJJe6hh+D77+1JVhXYXAn3ws5EFdqjhYjcA8QB7QpbboyZDkwHiIuL014xlFtN/GYiC7ct5MWbX6RFjRZOl+MIEXtz01nG2NdU4HGlWSYJqJXveU1gX8GVRKQTMBSIN8aku6c8pVzzxe4vGLxyMH9p+BeeuP4Jp8vxCpMnw1//qu3vgcqVcF8P1BOROiISBvQEFudfQUSaA69jg13P1asStf/kfnos6EHdinWZ2XWm31/26KqgIFiwwN7opAJPsc0yxpgsERkALAeCgZnGmC0iMhJIMMYsBiYAZYH3cv/H2mOMifdg3UoBkJ6Vzl8X/JWTGSf5tM+nlAsv53RJXmPAADuw9siRcO219nJJFThcGqzDGLMUWFrgtWH55ju5uS6limWM4cGPHmT1ntXM+8s8rqlyjdMleRURePVV2LoV+vSxJ1kbN3a6KlVS9A5V5bPGfT2lor55AAAOA0lEQVSO2ZtmM7zdcHo07uF0OV4pIsLetRoVpd0TBBodZk/5pPe3vs+QT4dwV+O7GNZuWPFvCGDVq8O2bRDp+93Yq4ugR+7K56zes5p7PriH1jVb6wlUF50N9k8/hbvvhqwsZ+tRnqfhrnzKpgOb+PM7fyamfAyLey4mIiTC6ZJ8SmKi7f/94Yd1/FV/p80yymckHk3klv/eQmR4JCt6r6BymcpOl+RzHnwQfvsNRo+GSpVg7Fi9yclfabgrn/Dr8V+5+a2bycrJYtW9q4gpr91XXKpRo+DIERg3DkJC7HMNeP+j4a683q5ju+gwuwMp6Sms6L2ChpUbOl2STxOBqVNtu3tSknZR4K803JVX23lsJx1md+Bk+klW9l7JddWvc7okvxAUBK+/njd/9ChUqKAh70/0hKryWluTt9JuVjtOZZzi0z6farC7WVBQXrC3aAGDBmk/NP5Ew115pdV7VtN2ZttzbezNqzV3uiS/FRUF8fHwyivQuzdkZDhdkXIHbZZRXueDbR9w98K7qV2+NsvuWUZsVKzTJfm1oCB46SW44goYMsSebJ0/H8ppNz0+TY/cldcwxvDC6hf4y/y/0KxqM1bft1qDvYSIwODB8MYbsHIlPKG9Jvs8PXJXXiE1I5X7Ft/H/C3z6dm4J2/Gv0np0NJOlxVw7r8f6taFRrmjJOfk2CN75Xv0a1OO23F0B3+c+UcWbF3A+E7jeaf7OxrsDmrXDipXhsxMuOUWGD9eT7T6Ig135RhjDG9teotmrzdjT8oelt69lKfbPK19xXiJjAx7svUf/4AuXeDgQacrUhdDw105IiUthV4Le9FnUR+aV23Opoc2cUvdW5wuS+VTpow9sfraa/Dll9C0KSxb5nRVylUa7qrE/W/7/2jyahPmb5nPqA6jtDsBLyZi+6NZv9421QwcaJtrlPfTE6qqxBw8dZDHlj/GvB/n0ahyI1bfuZrWNVs7XZZyQePGkJBgOx0LDYXTp+Gzz+C22/SuVm+lR+7K4zKzM3l53cs0nNqQhdsWMqL9CL5/8HsNdh8TEQH16tn5adPg9tuhc2c7jJ/yPnrkrjzGGMOS7Ut4esXTbD+ynY51OjKlyxTt+MsPDBpke5QcMcK2xT/8MAwfbrsRVt5Bj9yV2xljWLVrFR1md6DrvK4Iwkd3fcTK3is12P1EaCg89hj88ottk582Dfr1c7oqlZ8euSu3McawYucKRn4xkq9/+5pqZasxpcsU+l3Xj9DgUKfLUx4QHW27D37kkby29127YNYsePRRPZJ3kh65q8t2JvMMb373Js1fb84t/72FX1N+ZUqXKewctJP+LftrsAeAa67Ju6t12TIYORJq1oS//x1+/NHZ2gKVhru6ZL8c+YV/rPgHNSfV5IElD5Btspn+5+kkDkykf8v+Or5pgHr4YRvovXvDf/8LTZpA1646ZmtJ02YZdVGOnD7Cu1ve5a3Nb7E2aS3BEky3Bt0Y2HIgN9a+Ue8uVYA9kp8+3Y7ROmMGHDuW12zz/PPQqRO0aqWXUXqSGIf+nMbFxZmEhARHtq0uzr6T+1jy8xIWb1/Mih0ryMzJpEmVJvRu2pu7m9xNjXI1nC5R+YikJKhfH86cgZgY+L//s1PbthAc7HR1vkFENhhj4opdT8NdFZSZnUnCvgRW7lzJ4u2LSdhnv6crK1xJ9wbduafpPVxb9VqHq1S+6sQJWLjQTp98AunpMHcu9Oxp+5LPyIBq1Zyu0ntpuCuXpWels+ngJj7f/Tmrdq9i9Z7VnMo4hSC0qtmK+PrxxF8dT6PKjbTZRbnVqVPw8ce298ly5WDiRHjqKdusc9NN0KYNtG4NtWppE85ZGu6qUJnZmWw/sp31+9azfu961u9bz6aDm8jItmOrNYxuSIfYDnSo04F2tdtRuUxlhytWgWT7dvjwQztgyFdf2eabkBB7tF+qlO3yIDPT3jhVtWpgBr6Ge4BLzUhl1/FdbEvextbkrWxJ3sLW5K1sP7KdzBzb81NkWCRx1eNoUb0FLWq0oG1MW6qWrepw5UpZmZmwebO9UapnT/tax46wapWdj462V+K0bw/DhtnXkpOhYkX/br93NdxdulpGRDoDk4Fg4A1jzAsFlocDc4DrgCNAD2PM7ostWrkmOyebQ6mHOHDqAAdOHWBPyh52Hd/FruO72H18N7uO7SL5dPK59QXhygpXck2Va7i9/u1cU+Ua4qrHUb9SfYJEr4ZV3ik0FK67zk5nLVgAP/xgQ//stHFj3vK2bWH3brjySjuiVGysfa1HD7t8926oUgVKB8BYMMWGu4gEA1OBm4EkYL2ILDbG5O8u6H7gmDGmroj0BMYBPTxRsL/JzsnmeNpxjp45yrG0Yxw7c+zc49nXjpw5ci7I95/cT/LpZHLM+UPjhAaFUjuqNrFRsXRr0I06UXWIjYqlQXQDGkQ3oFRoKYf2UCn3qVjRjhTVrl3hy4cMgW3bIDHRHvGvXg0nT9pwNwYaNoS0NKhQwQ4IXrky3HWXvTY/JwcmT7a/CKKj7UAl5cpB9ep2fV/jypF7SyDRGLMTQETmAV2B/OHeFRieO78AmCIiYpxq8ymCMYZsk02OySE7J5tsk33eY47JITMnk4zsjHNTelZ63nx2epGvp2elczrzNKmZqZzKOEVqZiqpGannPZ7KOHVu/nTm6SLrLRVSioqlKlItsho1y9Ukrloc1SKrUbVsVaqVtY81y9WkemR1goP8+LeoUi7o2/f3r2Vl2cecHHj9ddi7116SeeiQbcY52z/98eOFDww+YoRt9tm3Dxo0sIF/dipbFgYMgG7dYP9+e2duqVK2B81SpezUpYs9QXz0KHzxhb0ENP+vEU9xJdxrAL/le54EtLrQOsaYLBFJASoBh91RZH4zv5/JhDUTigznol4zeP7vTbAEUyasDGVCy1A2rOy5+fIR5akeWd2+FlqGMmFliAyLpEKpClSIqECFUhWoWKriufkKERUIDwn3eL1K+bOQ3JQLDoY+fS68XoUK9marw4dt6Kek2BO5Z7tVCAuzA4ifOJE3nTyZ98fh6FF7eeeZM3Y6+0fliitsuG/ZAt27218R8+Z5bn/PciXcCzsfXTAhXVkHEekH9AOIibm0kXeiS0fTpEoTgoOCCZIggiWY4KBg+yi5r519nu/xd+sW8VpIUAjhweGEh4QTFhxGWHAY4cF582HBYRdcFh4STmhQqF4yqJSPEbFNMVFRtr2+oOhomDTpwu+/5przx5nNyrIhHxZmnzdvDt99Z4/4S4Ir4Z4E1Mr3vCaw7wLrJIlICFAeOFrwg4wx04HpYK+WuZSC46+211wrpZQ3CwmByMi852XL2oAvKa5cKrEeqCcidUQkDOgJLC6wzmLg3tz5O4DPvLG9XSmlAkWxR+65begDgOXYSyFnGmO2iMhIIMEYsxh4E3hLRBKxR+w9PVm0Ukqporl0nbsxZimwtMBrw/LNpwF3urc0pZRSl0rvYFFKKT+k4a6UUn5Iw10ppfyQhrtSSvkhDXellPJDjnX5KyLJwK+X+PZoPNC1gUN0X7yPv+wH6L54q8vZl9rGmGIHWnAs3C+HiCS40p+xL9B98T7+sh+g++KtSmJftFlGKaX8kIa7Ukr5IV8N9+lOF+BGui/ex1/2A3RfvJXH98Un29yVUkoVzVeP3JVSShXBq8NdRDqLyM8ikigigwtZHi4i7+YuXycisSVfpWtc2Je+IpIsIhtzpwecqLM4IjJTRA6JyI8XWC4i8nLufm4WkT+UdI2ucmFf2otISr7vZFhh6zlNRGqJyCoR2SYiW0RkUCHr+MT34uK++Mr3EiEi34rIptx9GVHIOp7LMGOMV07Y7oV3AFcCYcAmoFGBdR4BXsud7wm863Tdl7EvfYEpTtfqwr7cCPwB+PECy28FPsaOztUaWOd0zZexL+2Bj5yu04X9qAb8IXc+EtheyL8vn/heXNwXX/leBCibOx8KrANaF1jHYxnmzUfu5wbmNsZkAGcH5s6vKzA7d34BcJN45/h2ruyLTzDGfEkho2zl0xWYY6y1QJSIVCuZ6i6OC/viE4wx+40x3+XOnwS2Ycc1zs8nvhcX98Un5P63PpX7NDR3KniS02MZ5s3hXtjA3AW/5PMG5gbODsztbVzZF4C/5P5kXiAitQpZ7gtc3VdfcX3uz+qPReQap4spTu7P+ubYo8T8fO57KWJfwEe+FxEJFpGNwCFghTHmgt+LuzPMm8PdbQNzewFX6lwCxBpjmgIryftr7mt85TtxxXfYW72vBV4BFjlcT5FEpCzwPvCYMeZEwcWFvMVrv5di9sVnvhdjTLYxphl27OmWItK4wCoe+168OdwvZmBuihqY2wsUuy/GmCPGmPTcpzOA60qoNndz5XvzCcaYE2d/Vhs7GlmoiEQ7XFahRCQUG4ZvG2MWFrKKz3wvxe2LL30vZxljjgOfA50LLPJYhnlzuPvTwNzF7kuB9s94bFujL1oM9Mm9OqM1kGKM2e90UZdCRKqebf8UkZbY/1+OOFvV7+XW+CawzRjz0gVW84nvxZV98aHvpbKIROXOlwI6AT8VWM1jGebSGKpOMH40MLeL+/KoiMQDWdh96etYwUUQkbnYqxWiRSQJeA57oghjzGvYsXZvBRKB08DfnKm0eC7syx3AwyKSBZwBenrpwUMboDfwQ277LsCzQAz43Pfiyr74yvdSDZgtIsHYP0DzjTEflVSG6R2qSinlh7y5WUYppdQl0nBXSik/pOGulFJ+SMNdKaX8kIa7Ukr5IQ13pZTyQxruSinlhzTclVLKD/0/8tJFRBJdYDoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# estimated probs\n",
    "\n",
    "X_new = np.linspace(0, 3, 1000).reshape(-1, 1)\n",
    "y_proba = log_reg.predict_proba(X_new)\n",
    "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\")\n",
    "plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here how the most \"confident\" predictions are at the edges of the graph, while the center values (around 1.6 cm) are where the uncertainty is the highest (both around 50%).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: hyperparameter for regularization of Log Reg is not `alpha` but $\\frac{1}{\\alpha} =$ `C` . Higher C > Less regularized the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression\n",
    "\n",
    "Its other name (Multinomial Logistic Regression) basically explains what is it, a Log Reg generalized to multiple classes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
