{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II. Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Introduction to Artificial Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial Neural Networks (ANN) is a Machine Learning model inspired by the networks of biological neurons found in our brains.  \n",
    "\n",
    "Although they draw from our biological brains, they have slightly evolved to be somehow different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it different this time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANNs have been around for quite some time, with efforts going back to a seminal paper by McCulloch and Pitts (1943). However, after a long winter started in the 1960s they seem to be back in town as the cool kid. \n",
    "\n",
    "Why would this time be different? According to the author:\n",
    "\n",
    "**1. Data quantity**, which allows ANNs to perform traditional ML on large and complex problems\n",
    "\n",
    "**2. Computing power**, thanks to Moore's Law, the gaming industry (for GPUs) and cloud computing \n",
    "\n",
    "**3. Improved algorithms** (not that different from 1990s, but those differences had huge impact)\n",
    "\n",
    "**4. Theoretical limitations** (e.g. getting stuck in local optima) are **rather rare** in practice, or **not as serious** as previously thought\n",
    "\n",
    "**5. Virtual cycle** of applications > reaserch + funding > more and better applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logical Computations with Neurons\n",
    "\n",
    "_**Note**: Skipping the part on biological neurons_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "McCulloch and Pitts proposed a simple model of the biological neuron later known as an **artificial neuron** characterized by one or more binary inputs and one binary output. \n",
    "\n",
    "Even with this simple neuron, it is possible to build an ANN that computes any logical proposition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ANN](images/10.ANN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming an activation threshold of two, we can see from the picture above how this could work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in complexity is the _Perceptron_. Invented in 1957 by Frank Rosenblatt, it is based on an artificial neuron called a _threshold logic unit_ (TLU) / _linear threshold unit_ (LTU). Inputs and outputs are numbers and each input is connected with a weight.\n",
    "\n",
    "TLU computes a weighted sum and then applies a step function to the sum:\n",
    "\n",
    "1. $z = w_1x_1 + w_2n_2 + \\cdots + w_nx_n = x^tw$\n",
    "\n",
    "2. $h_w(x) = step(z) = step(x^tw)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Threshold Logic Unit](images/10.TLU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two common step functions used in Perceptrons:\n",
    "\n",
    "1. Heaviside (z) $ = \\begin{cases}\n",
    "0 & z < 0\\\\\n",
    "1 & z \\ge 0\n",
    "\\end{cases}$\n",
    "\n",
    "2. Sign (z) $ = \\begin{cases}\n",
    "-1 & z < 0 \\\\\n",
    "0 & z = 0 \\\\\n",
    "+1 & z > 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single TLU can be used for linear binary classification (using a threshold, similarly to LogReg or SVM).  \n",
    "A Perceptron is simply a single layer of TLUs, with each TLU connected to all the inputs. Having multiple output TLU makes possible to perform multioutput classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is then possible to compute the outputs of a layer of artificial neurons for several instances at once:\n",
    "\n",
    "$h_{W,b} (X) = \\phi (XW + b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X$ = matrix of input features  \n",
    "\n",
    "$W$ = weights of neurons (expept bias). One row per input neuron and one col for artificial neuron. \n",
    "\n",
    "$b$ = vector containing all the connection weights between the bias neuron and the artificial neurons. It has one bias term per artificial neuron.\n",
    "\n",
    "$\\phi$ = activation function. In our case (artificial neuron = TLU), the activation function is a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "The Perceptron is then trained reinforcing connections that help **reduce the error**. \n",
    "\n",
    "More specifically, the Perceptron is fed one training instance at a time, and for each instance it makes its predictions. For every output neuron that produced a wrong prediction, it reinforces the connection weights from the inputs that would have contributed to the correct prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally:\n",
    "\n",
    "$w_{i,y}^{(next step)} = w_{i,y} + \\eta (y_j - \\hat{y}_j)x_i $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w_{i,y}$ = connection weight between $i^{th}$ input neuron and $j^{th}$ output neuron\n",
    "\n",
    "* $x_i$ is the $i^{th}$ input value of the current training instance\n",
    "\n",
    "* $\\hat{y}_j$ is the output of the $j^{th}$ output neuron for the current training instance\n",
    "\n",
    "* $y_j$ is the target output of the $j^{th}$ output neuron for the current training instance\n",
    "\n",
    "* $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the Perceptron decision boundaries are linear, but as long as training instances are linearly separable, it will converge to a solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron is a fairly rudimentary ANN architecture, incapable for example to solve the exclusive or (XOR) classification problem. \n",
    "\n",
    "It turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons: the resulting ANN is known as **Multilayer Perceptron (MLP)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron and Backpropagation\n",
    "\n",
    "An MLP is composed of multiple layers, generally named *input* (lower) layers, *hidden* layers, *output* (upper) layers. \n",
    "\n",
    "**Note**: so far we are only dealing with feedforward neural network (FNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation comes into play as a training method for MLP. In short, it computes the the gradient of the network’s error with regard to every single model parameter by going forward and then backwards. It does so many times until it converges. \n",
    "\n",
    "**Note**: More specifically, automatically computing gradients is called automatic differentiation, or **autodiff**. The autodiff technique used by backpropagation is called **reverse-mode autodiff**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run through the algorithm:\n",
    "\n",
    "1. **Forward pass**: each mini-batch of training instances is passed to the network’s input layer > hidden layers > output layers\n",
    "\n",
    "2. **Error measurement**: network error computed using a loss function that compares the desired output and the actual output of the network\n",
    "\n",
    "3. **Error attribution**: it computes how much each output connection contributed to the error\n",
    "\n",
    "4. **Reverse pass**: tracing back error from output layer to lower layer down to inputs\n",
    "\n",
    "5. **Gradient Descent**: tweak all the connection weights in the network, using the error gradients computed earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: it is important to randomize the initial hiddent layers weights (_break the simmetry_). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for this to work, David Rumelhart, Geoffrey Hinton, and Ronald Williams replaced the step function with the logistic (sigmoid) function: \n",
    "\n",
    "$\\displaystyle \\sigma(z) = \\frac{1}{1+ e^{(-z)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was particularly helpful for Gradient Descent, since with a stepwise function there is no gradient to work with. Secondly, having a non-linear function (and a chain of non-linear function over many layers) allows us to theoretically approximate any continuous function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression MLPs\n",
    "\n",
    "We can use MLPs for single variable regression tasks (with a single output neuron) or multivariable regression (with multiple output neurons). \n",
    "\n",
    "A typical regression MLP architecture consists of the following **hyperparameters**:\n",
    "* n. Input neurons\n",
    "* n. Hidden layers\n",
    "* n. Neurons per hidden layer\n",
    "* n. Output neurons\n",
    "* Hidden activation function\n",
    "* Output activation function\n",
    "* Loss activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification MLPs\n",
    "\n",
    "For a binary classification problem, you just need a single output neuron using the logistic activation function: the output will be a number between 0 and 1, which you can interpret as the estimated probability of the positive class. \n",
    "\n",
    "MLP classification **hyperparameters**:\n",
    "* n. Input neurons\n",
    "* n. Hidden layers\n",
    "* n. Output neurons\n",
    "* Output layer activation (log / softmax)\n",
    "* Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing MLPs with Keras\n",
    "\n",
    "[Keras](https://github.com/keras-team/keras) is a high-level Deep Learning API. On the backend, it relies on one of three popular open source Deep Learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano.\n",
    "\n",
    "**Note**: for simplicity we will use the Tensorflow implementation `tf.keras` (without any specific TF features) therefore altough the code in this chapter can be used on any backend implementation (_usually by changing the imports_).\n",
    "\n",
    "First thing, let's make sure that [Tensorflow](https://www.tensorflow.org/) is up and running (installing it in a virtualenv is recommended):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Image Classifier Using the Sequential API\n",
    "\n",
    "Let's use the Fashion MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 110s 4us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0s/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 7s 2us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shape\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type\n",
    "X_train_full.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a validation set. Also, input scales since we will be working with Gradient Descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "\"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Sequential model (simplest NN)\n",
    "model = keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer - no pars - preprocessing\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "# first Dense hidden layer with 300 neurons\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second with 100 neurons\n",
    "model.add(keras.layers.Dense(100, activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer with 10 neurons - softmax act function since we have exclusive classes\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model summary (default layer names)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to _break simmetry_ all the connection weights have been initialized randomly. We can change the initialization method by setting a specific `kernel_initializer`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the method\n",
    "\n",
    "After a model is created, you must call its `compile()` method to specify the loss function and the optimizer to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", # because we have sparse labels\n",
    "              optimizer=\"sgd\", # stochastic gradient descent - generally we would also set a learning rate (def = 0.01)\n",
    "              metrics=[\"accuracy\"]) # because it's a class problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 15s 281us/sample - loss: 0.7118 - acc: 0.7629 - val_loss: 0.4968 - val_acc: 0.8374\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 12s 213us/sample - loss: 0.4884 - acc: 0.8299 - val_loss: 0.4451 - val_acc: 0.8436\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 12s 224us/sample - loss: 0.4434 - acc: 0.8437 - val_loss: 0.4114 - val_acc: 0.8602\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 15s 274us/sample - loss: 0.4170 - acc: 0.8538 - val_loss: 0.3982 - val_acc: 0.8650\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 12s 218us/sample - loss: 0.3966 - acc: 0.8597 - val_loss: 0.4369 - val_acc: 0.8458\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 12s 213us/sample - loss: 0.3800 - acc: 0.8653 - val_loss: 0.3703 - val_acc: 0.8698\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 17s 310us/sample - loss: 0.3663 - acc: 0.8693 - val_loss: 0.3699 - val_acc: 0.8708\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 14s 253us/sample - loss: 0.3544 - acc: 0.8741 - val_loss: 0.3702 - val_acc: 0.8652\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 12s 222us/sample - loss: 0.3436 - acc: 0.8775 - val_loss: 0.3449 - val_acc: 0.8778\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 15s 276us/sample - loss: 0.3340 - acc: 0.8819 - val_loss: 0.3389 - val_acc: 0.8782\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 13s 239us/sample - loss: 0.3255 - acc: 0.8839 - val_loss: 0.3495 - val_acc: 0.8742\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 14s 246us/sample - loss: 0.3174 - acc: 0.8866 - val_loss: 0.3417 - val_acc: 0.8806\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 14s 251us/sample - loss: 0.3100 - acc: 0.8893 - val_loss: 0.3255 - val_acc: 0.8836\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 13s 238us/sample - loss: 0.3041 - acc: 0.8909 - val_loss: 0.3233 - val_acc: 0.8822\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 15s 275us/sample - loss: 0.2962 - acc: 0.8919 - val_loss: 0.3180 - val_acc: 0.8850\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 15s 266us/sample - loss: 0.2910 - acc: 0.8946 - val_loss: 0.3202 - val_acc: 0.8870\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 14s 247us/sample - loss: 0.2846 - acc: 0.8974 - val_loss: 0.3300 - val_acc: 0.8798\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 15s 274us/sample - loss: 0.2787 - acc: 0.8998 - val_loss: 0.3245 - val_acc: 0.8810\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 18s 330us/sample - loss: 0.2734 - acc: 0.9013 - val_loss: 0.3029 - val_acc: 0.8910\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 18s 333us/sample - loss: 0.2682 - acc: 0.9023 - val_loss: 0.3219 - val_acc: 0.8810\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 18s 331us/sample - loss: 0.2636 - acc: 0.9044 - val_loss: 0.3130 - val_acc: 0.8878\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 18s 322us/sample - loss: 0.2580 - acc: 0.9066 - val_loss: 0.3131 - val_acc: 0.8864\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 12s 224us/sample - loss: 0.2550 - acc: 0.9084 - val_loss: 0.3039 - val_acc: 0.8898\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 17s 300us/sample - loss: 0.2491 - acc: 0.9101 - val_loss: 0.3104 - val_acc: 0.8856\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 16s 297us/sample - loss: 0.2452 - acc: 0.9123 - val_loss: 0.3044 - val_acc: 0.8904\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 15s 273us/sample - loss: 0.2412 - acc: 0.9130 - val_loss: 0.3131 - val_acc: 0.8856\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 14s 256us/sample - loss: 0.2375 - acc: 0.9148 - val_loss: 0.3158 - val_acc: 0.8834\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 15s 281us/sample - loss: 0.2342 - acc: 0.9156 - val_loss: 0.2954 - val_acc: 0.8974\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 16s 295us/sample - loss: 0.2281 - acc: 0.9175 - val_loss: 0.2959 - val_acc: 0.8902\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 16s 285us/sample - loss: 0.2257 - acc: 0.9186 - val_loss: 0.3041 - val_acc: 0.8930\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy **89.30%**, not bad! \n",
    "\n",
    "**Note**: if the training set was skewed, it would be useful to set the `class_weight` argument when calling the `fit()` method, which would give a larger weight to underrepresented classes and a lower weight to overrepresented classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we are not satisfied with the performance of our model, there are many things we could do:\n",
    "\n",
    "* Learning rate\n",
    "* Change optimizer\n",
    "* N. of layers\n",
    "* N. of neurons per layer \n",
    "* Activation functions\n",
    "* Batch size\n",
    "\n",
    "etc.\n",
    "\n",
    "Once we are satisfied with **validation** accuracy we can evaluate it on the **test** set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 77us/sample - loss: 77.1612 - acc: 0.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[77.16121282806397, 0.8327]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairly easy to interpret: e.g. 1 we have a 100% prob of class 10, 0% of any other class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression MLP using Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s switch to the California housing problem and tackle it using a regression neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\giuse\\scikit_learn_data\n",
      "INFO:sklearn.datasets.california_housing:Downloading Cal. housing from https://ndownloader.figshare.com/files/5976036 to C:\\Users\\giuse\\scikit_learn_data\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference here is that we only have one single neuron in the output layer and no activation function.\n",
    "\n",
    "Since the dataset is quite noisy, we just use a single hidden layer with fewer neurons than before, to avoid overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 1.1655 - val_loss: 0.5259\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5076 - val_loss: 0.5084\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4671 - val_loss: 0.4615\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4487 - val_loss: 0.4514\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4373 - val_loss: 0.4514\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.4417 - val_loss: 0.4289\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4252 - val_loss: 0.4278\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4171 - val_loss: 0.4239\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4111 - val_loss: 0.4178\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4091 - val_loss: 0.4171\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.4016 - val_loss: 0.4307\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3996 - val_loss: 0.3990\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3941 - val_loss: 0.3965\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.3872 - val_loss: 0.4083\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: 0.3945 - val_loss: 0.3930\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: 0.3788 - val_loss: 0.3877\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: 0.3812 - val_loss: 0.3902\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3747 - val_loss: 0.3843\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3700 - val_loss: 0.4338\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.3703 - val_loss: 1.4091\n",
      "5160/5160 [==============================] - 0s 15us/sample - loss: 1.0626\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\",\n",
    "input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Complex Models Using the Functional API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Sequential API is quite easy to use, we may need to build models with more complex topologies. Here comes the Functional API to the rescue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One such example is the _wide and deep_ NN, which connects all or part of the inputs directly to the output layer, allowing our network to learn both deep patterns and simple patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input object\n",
    "input_ = keras.layers.Input(shape=X_train.shape[1:]) \n",
    "# dense layer with 30 neurons\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "# dense layer with 30 neurons (input is hidden1)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "# concatenate layer (input + hidden2 output)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "# output layer (single layer & no activation function)\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "# create the model specifying inputs and outputs\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step here is to understand how to send a subset of features to the wide net, and a subset to the deep net. To do this, we will use multiple inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create our input matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 1.8344 - val_loss: 0.8213\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: 0.7004 - val_loss: 0.6745\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.6209 - val_loss: 0.6102\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.5854 - val_loss: 0.5818\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5637 - val_loss: 0.5607\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5491 - val_loss: 0.5487\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.5357 - val_loss: 0.5355\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.5252 - val_loss: 0.5253\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.5168 - val_loss: 0.5186\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: 0.5084 - val_loss: 0.5176\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.5021 - val_loss: 0.5060\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.4969 - val_loss: 0.5013\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4918 - val_loss: 0.4957\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 39us/sample - loss: 0.4876 - val_loss: 0.4917\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4841 - val_loss: 0.4884\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 33us/sample - loss: 0.4809 - val_loss: 0.4855\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4779 - val_loss: 0.4824\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.4752 - val_loss: 0.4808\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 29us/sample - loss: 0.4718 - val_loss: 0.4765\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 30us/sample - loss: 0.4684 - val_loss: 0.4727\n",
      "5160/5160 [==============================] - 0s 14us/sample - loss: 0.5079\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding extra outputs to the NN is also quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "\n",
    "# connect to appropriate layers\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output,\n",
    "aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output will have its own loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we train the model, we need to provide labels for each output. In this example, the main output and the auxiliary output should try to predict the same thing, so they should use the same labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.9901 - main_output_loss: 0.8792 - aux_output_loss: 1.9866 - val_loss: 0.6264 - val_main_output_loss: 0.5634 - val_aux_output_loss: 1.1922\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.6089 - main_output_loss: 0.5586 - aux_output_loss: 1.0606 - val_loss: 0.5546 - val_main_output_loss: 0.5068 - val_aux_output_loss: 0.9853\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.5288 - main_output_loss: 0.4900 - aux_output_loss: 0.8773 - val_loss: 0.5128 - val_main_output_loss: 0.4751 - val_aux_output_loss: 0.8525\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4969 - main_output_loss: 0.4644 - aux_output_loss: 0.7882 - val_loss: 0.4980 - val_main_output_loss: 0.4665 - val_aux_output_loss: 0.7823\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4791 - main_output_loss: 0.4510 - aux_output_loss: 0.7330 - val_loss: 0.4830 - val_main_output_loss: 0.4564 - val_aux_output_loss: 0.7236\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.4687 - main_output_loss: 0.4440 - aux_output_loss: 0.6898 - val_loss: 0.4665 - val_main_output_loss: 0.4407 - val_aux_output_loss: 0.6985\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.4567 - main_output_loss: 0.4339 - aux_output_loss: 0.6614 - val_loss: 0.4580 - val_main_output_loss: 0.4351 - val_aux_output_loss: 0.6623\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4444 - main_output_loss: 0.4229 - aux_output_loss: 0.6372 - val_loss: 0.4473 - val_main_output_loss: 0.4263 - val_aux_output_loss: 0.6368\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 35us/sample - loss: 0.4344 - main_output_loss: 0.4144 - aux_output_loss: 0.6145 - val_loss: 0.4374 - val_main_output_loss: 0.4156 - val_aux_output_loss: 0.6350\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4246 - main_output_loss: 0.4056 - aux_output_loss: 0.5943 - val_loss: 0.4217 - val_main_output_loss: 0.4014 - val_aux_output_loss: 0.6042\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 34us/sample - loss: 0.4162 - main_output_loss: 0.3982 - aux_output_loss: 0.5808 - val_loss: 0.4203 - val_main_output_loss: 0.4011 - val_aux_output_loss: 0.5942\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.4088 - main_output_loss: 0.3915 - aux_output_loss: 0.5644 - val_loss: 0.4109 - val_main_output_loss: 0.3937 - val_aux_output_loss: 0.5674\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.4009 - main_output_loss: 0.3849 - aux_output_loss: 0.5435 - val_loss: 0.4067 - val_main_output_loss: 0.3868 - val_aux_output_loss: 0.5870\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3931 - main_output_loss: 0.3776 - aux_output_loss: 0.5327 - val_loss: 0.3937 - val_main_output_loss: 0.3776 - val_aux_output_loss: 0.5373\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3833 - main_output_loss: 0.3679 - aux_output_loss: 0.5222 - val_loss: 0.3854 - val_main_output_loss: 0.3694 - val_aux_output_loss: 0.5297\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3763 - main_output_loss: 0.3615 - aux_output_loss: 0.5094 - val_loss: 0.3920 - val_main_output_loss: 0.3774 - val_aux_output_loss: 0.5238\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3759 - main_output_loss: 0.3618 - aux_output_loss: 0.5025 - val_loss: 0.3789 - val_main_output_loss: 0.3618 - val_aux_output_loss: 0.5316\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3656 - main_output_loss: 0.3516 - aux_output_loss: 0.4910 - val_loss: 0.3715 - val_main_output_loss: 0.3572 - val_aux_output_loss: 0.5001\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3643 - main_output_loss: 0.3513 - aux_output_loss: 0.4810 - val_loss: 0.3743 - val_main_output_loss: 0.3571 - val_aux_output_loss: 0.5289\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3613 - main_output_loss: 0.3481 - aux_output_loss: 0.4788 - val_loss: 0.3606 - val_main_output_loss: 0.3467 - val_aux_output_loss: 0.4862\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation using total loss and individual losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5160/5160 [==============================] - 0s 23us/sample - loss: 0.3834 - main_output_loss: 0.3689 - aux_output_loss: 0.5004\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subclassing API for Dynamic Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, we can construct a `Model` class, create our layers in the constructor, and use them to perform the computations you want in the `call()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method makes our model much more customizable, but also less easy to inspect. We can also stack up several models together to form more complex models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard for Visualization\n",
    "\n",
    "TensorBoard is an interactive visualization tool useful to: \n",
    "* View the learning curves during training\n",
    "* Compare learning curves between multiple runs\n",
    "* Visualize the computation graph\n",
    "* Analyze training statistics\n",
    "* View images generated by the model\n",
    "* Visualize complex multidimensional data projected down to 3D and automatically clustered \n",
    "\n",
    ".. and more .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But**, to use we need to modify our program so that the input is in special binary log files called _event files_. The TensorBoard server will monitor the log\n",
    "directory, and it will automatically pick up the changes and update the visualizations. \n",
    "\n",
    "Ideally, you want the server to point to a root log directory and configure your program so that it writes to a different subdirectory every time it runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining root log directory\n",
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subdir based on time (it may be helpful to add hyperparams and other details)\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = get_run_logdir() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the TensorBoard server. While there is also an option to start it directly from the CLI, I prefer to launch it from Jupyter itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18224), started 0:04:00 ago. (Use '!kill 18224' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a1731b6f3a7021cc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a1731b6f3a7021cc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this brief introduction on NN, it's time to jump a level deeper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Neural Network Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One option to find the best hyperparameters is to run random combinations, e.g. using `GridSearchCV`. To do this, we need to wrap our Keras models in objects that mimic regular Scikit-Learn regressors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3,\n",
    "input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr=learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a simple Sequential model for univariate regression (only one output neuron), with the given input shape and the given number of hidden layers and neurons, and it compiles it using an SGD optimizer configured with the specified learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create a `KerasRegressor` based on this `build_model()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this object like a regular Scikit-learn regressor: train it with `fit()`, evaluate it with `score()` and use it to make predictions with `predict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 117us/sample - loss: 1.2916 - val_loss: 0.7770\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.6912 - val_loss: 0.6150\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.6283 - val_loss: 0.6211\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.5408 - val_loss: 0.5206\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.5101 - val_loss: 0.5022\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.4871 - val_loss: 0.4917\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4747 - val_loss: 0.4806\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - ETA: 0s - loss: 0.462 - 1s 56us/sample - loss: 0.4646 - val_loss: 0.4865\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4592 - val_loss: 0.4622\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4509 - val_loss: 0.4751\n",
      "Epoch 11/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4507 - val_loss: 0.4512\n",
      "Epoch 12/100\n",
      "11610/11610 [==============================] - 1s 60us/sample - loss: 0.4421 - val_loss: 0.4531\n",
      "Epoch 13/100\n",
      "11610/11610 [==============================] - 1s 72us/sample - loss: 0.4411 - val_loss: 0.4468\n",
      "Epoch 14/100\n",
      "11610/11610 [==============================] - 1s 64us/sample - loss: 0.4377 - val_loss: 0.4430\n",
      "Epoch 15/100\n",
      "11610/11610 [==============================] - 1s 62us/sample - loss: 0.4320 - val_loss: 0.4426\n",
      "Epoch 16/100\n",
      "11610/11610 [==============================] - 1s 61us/sample - loss: 0.4346 - val_loss: 0.4357\n",
      "Epoch 17/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.4289 - val_loss: 0.4354\n",
      "Epoch 18/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4236 - val_loss: 0.4311\n",
      "Epoch 19/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.4213 - val_loss: 0.4276\n",
      "Epoch 20/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4267 - val_loss: 0.4251\n",
      "Epoch 21/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4189 - val_loss: 0.4238\n",
      "Epoch 22/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4186 - val_loss: 0.4211\n",
      "Epoch 23/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.4147 - val_loss: 0.4257\n",
      "Epoch 24/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4377 - val_loss: 0.4237\n",
      "Epoch 25/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4106 - val_loss: 0.4208\n",
      "Epoch 26/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4098 - val_loss: 0.4180\n",
      "Epoch 27/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4086 - val_loss: 0.4151\n",
      "Epoch 28/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.4092 - val_loss: 0.4141\n",
      "Epoch 29/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.4039 - val_loss: 0.4114\n",
      "Epoch 30/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.4036 - val_loss: 0.4117\n",
      "Epoch 31/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4017 - val_loss: 0.4092\n",
      "Epoch 32/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3993 - val_loss: 0.4071\n",
      "Epoch 33/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3980 - val_loss: 0.4078\n",
      "Epoch 34/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3978 - val_loss: 0.4114\n",
      "Epoch 35/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.4144 - val_loss: 0.4042\n",
      "Epoch 36/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3962 - val_loss: 0.4046\n",
      "Epoch 37/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3950 - val_loss: 0.4058\n",
      "Epoch 38/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3924 - val_loss: 0.4016\n",
      "Epoch 39/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3917 - val_loss: 0.4007\n",
      "Epoch 40/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3939 - val_loss: 0.4029\n",
      "Epoch 41/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3885 - val_loss: 0.3963\n",
      "Epoch 42/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3898 - val_loss: 0.3963\n",
      "Epoch 43/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3869 - val_loss: 0.3989\n",
      "Epoch 44/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3867 - val_loss: 0.3946\n",
      "Epoch 45/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3840 - val_loss: 0.3925\n",
      "Epoch 46/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3830 - val_loss: 0.3917\n",
      "Epoch 47/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3821 - val_loss: 0.3909\n",
      "Epoch 48/100\n",
      "11610/11610 [==============================] - 1s 59us/sample - loss: 0.3815 - val_loss: 0.3912\n",
      "Epoch 49/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3810 - val_loss: 0.3925\n",
      "Epoch 50/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3835 - val_loss: 0.3873\n",
      "Epoch 51/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3799 - val_loss: 0.3863\n",
      "Epoch 52/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3760 - val_loss: 0.3842\n",
      "Epoch 53/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3742 - val_loss: 0.3866\n",
      "Epoch 54/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3737 - val_loss: 0.3829\n",
      "Epoch 55/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3719 - val_loss: 0.3829\n",
      "Epoch 56/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3725 - val_loss: 0.3821\n",
      "Epoch 57/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3697 - val_loss: 0.3817\n",
      "Epoch 58/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3689 - val_loss: 0.3812\n",
      "Epoch 59/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3706 - val_loss: 0.3805\n",
      "Epoch 60/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3690 - val_loss: 0.3799\n",
      "Epoch 61/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3684 - val_loss: 0.3772\n",
      "Epoch 62/100\n",
      "11610/11610 [==============================] - 0s 42us/sample - loss: 0.3657 - val_loss: 0.3769\n",
      "Epoch 63/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3646 - val_loss: 0.3734\n",
      "Epoch 64/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3638 - val_loss: 0.3756\n",
      "Epoch 65/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3854 - val_loss: 0.3766\n",
      "Epoch 66/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3609 - val_loss: 0.3776\n",
      "Epoch 67/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3598 - val_loss: 0.3715\n",
      "Epoch 68/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3597 - val_loss: 0.3713\n",
      "Epoch 69/100\n",
      "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3581 - val_loss: 0.3712\n",
      "Epoch 70/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3569 - val_loss: 0.3691\n",
      "Epoch 71/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3552 - val_loss: 0.3690\n",
      "Epoch 72/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3547 - val_loss: 0.3705\n",
      "Epoch 73/100\n",
      "11610/11610 [==============================] - 1s 48us/sample - loss: 0.3545 - val_loss: 0.3690\n",
      "Epoch 74/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3540 - val_loss: 0.3707\n",
      "Epoch 75/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3549 - val_loss: 0.3671\n",
      "Epoch 76/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3517 - val_loss: 0.3671\n",
      "Epoch 77/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3520 - val_loss: 0.3653\n",
      "Epoch 78/100\n",
      "11610/11610 [==============================] - 1s 51us/sample - loss: 0.3516 - val_loss: 0.3652\n",
      "Epoch 79/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3494 - val_loss: 0.3674\n",
      "Epoch 80/100\n",
      "11610/11610 [==============================] - 1s 52us/sample - loss: 0.3503 - val_loss: 0.3634\n",
      "Epoch 81/100\n",
      "11610/11610 [==============================] - 1s 49us/sample - loss: 0.3500 - val_loss: 0.3618\n",
      "Epoch 82/100\n",
      "11610/11610 [==============================] - 1s 58us/sample - loss: 0.3471 - val_loss: 0.3620\n",
      "Epoch 83/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3474 - val_loss: 0.3614\n",
      "Epoch 84/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3464 - val_loss: 0.3601\n",
      "Epoch 85/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3448 - val_loss: 0.3617\n",
      "Epoch 86/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3460 - val_loss: 0.3621\n",
      "Epoch 87/100\n",
      "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3436 - val_loss: 0.3617\n",
      "Epoch 88/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3466 - val_loss: 0.3632\n",
      "Epoch 89/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3434 - val_loss: 0.3600\n",
      "Epoch 90/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3421 - val_loss: 0.3590\n",
      "Epoch 91/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3407 - val_loss: 0.3578\n",
      "Epoch 92/100\n",
      "11610/11610 [==============================] - 1s 57us/sample - loss: 0.3422 - val_loss: 0.3596\n",
      "Epoch 93/100\n",
      "11610/11610 [==============================] - 1s 63us/sample - loss: 0.3391 - val_loss: 0.3552\n",
      "Epoch 94/100\n",
      "11610/11610 [==============================] - 1s 50us/sample - loss: 0.3385 - val_loss: 0.3552\n",
      "Epoch 95/100\n",
      "11610/11610 [==============================] - 1s 55us/sample - loss: 0.3372 - val_loss: 0.3583\n",
      "Epoch 96/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3380 - val_loss: 0.3567\n",
      "Epoch 97/100\n",
      "11610/11610 [==============================] - 1s 53us/sample - loss: 0.3381 - val_loss: 0.3584\n",
      "Epoch 98/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3414 - val_loss: 0.3610\n",
      "Epoch 99/100\n",
      "11610/11610 [==============================] - 1s 54us/sample - loss: 0.3411 - val_loss: 0.3539\n",
      "Epoch 100/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: 0.3355 - val_loss: 0.3521\n",
      "5160/5160 [==============================] - 0s 25us/sample - loss: 0.3712\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can use randomized search to see which variant perform best. Let's focus on # hidden layers, # neurons and learning rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 121us/sample - loss: 4.2837 - val_loss: 3.0196\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 57us/sample - loss: 2.2008 - val_loss: 1.7651\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 56us/sample - loss: 1.4705 - val_loss: 1.2920\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 1.1641 - val_loss: 1.0691\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.9878 - val_loss: 0.9372\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 1s 70us/sample - loss: 0.8804 - val_loss: 0.8587\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 1s 86us/sample - loss: 0.8130 - val_loss: 0.8106\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.7698 - val_loss: 0.7788\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 1s 73us/sample - loss: 0.7402 - val_loss: 0.7556\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.7186 - val_loss: 0.7374\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.7016 - val_loss: 0.7221\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.6875 - val_loss: 0.7087\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 1s 91us/sample - loss: 0.6752 - val_loss: 0.6968\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 1s 78us/sample - loss: 0.6641 - val_loss: 0.6858\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.6540 - val_loss: 0.6753\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.6447 - val_loss: 0.6657\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: 0.6360 - val_loss: 0.6565\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 1s 73us/sample - loss: 0.6279 - val_loss: 0.6481\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 0.6201 - val_loss: 0.6397\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.6130 - val_loss: 0.6323\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.6061 - val_loss: 0.6252\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.5995 - val_loss: 0.6183\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5933 - val_loss: 0.6117\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 1s 81us/sample - loss: 0.5874 - val_loss: 0.6057\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.5818 - val_loss: 0.6002\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 1s 70us/sample - loss: 0.5765 - val_loss: 0.5943\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.5714 - val_loss: 0.5888\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 1s 73us/sample - loss: 0.5664 - val_loss: 0.5836\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5617 - val_loss: 0.5792\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5573 - val_loss: 0.5747\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.5530 - val_loss: 0.5702\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5488 - val_loss: 0.5661\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5448 - val_loss: 0.5617\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 0.5410 - val_loss: 0.5581\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.5373 - val_loss: 0.5539\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.5337 - val_loss: 0.5505\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: 0.5303 - val_loss: 0.5469\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 1s 81us/sample - loss: 0.5270 - val_loss: 0.5436\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 1s 91us/sample - loss: 0.5238 - val_loss: 0.5405\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 0.5207 - val_loss: 0.5374\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5177 - val_loss: 0.5349\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 1s 96us/sample - loss: 0.5148 - val_loss: 0.5322\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 1s 70us/sample - loss: 0.5121 - val_loss: 0.5293\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5094 - val_loss: 0.5267\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.5069 - val_loss: 0.5241\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 1s 77us/sample - loss: 0.5044 - val_loss: 0.5219\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 1s 75us/sample - loss: 0.5020 - val_loss: 0.5198\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 1s 78us/sample - loss: 0.4998 - val_loss: 0.5171\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: 0.4977 - val_loss: 0.5151\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 1s 71us/sample - loss: 0.4955 - val_loss: 0.5128\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 1s 73us/sample - loss: 0.4935 - val_loss: 0.5110\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 1s 68us/sample - loss: 0.4914 - val_loss: 0.5092\n",
      "Epoch 53/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: 0.4895 - val_loss: 0.5072\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 1s 70us/sample - loss: 0.4877 - val_loss: 0.5058\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: 0.4859 - val_loss: 0.5040\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 0.4842 - val_loss: 0.5023\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 1s 76us/sample - loss: 0.4825 - val_loss: 0.5008\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 1s 66us/sample - loss: 0.4808 - val_loss: 0.4991\n",
      "Epoch 59/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: 0.4792 - val_loss: 0.4973\n",
      "Epoch 60/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: 0.4776 - val_loss: 0.4965\n",
      "Epoch 61/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: 0.4762 - val_loss: 0.4950\n",
      "Epoch 62/100\n",
      "7740/7740 [==============================] - 0s 59us/sample - loss: 0.4747 - val_loss: 0.4931\n",
      "Epoch 63/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4733 - val_loss: 0.4916\n",
      "Epoch 64/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: 0.4719 - val_loss: 0.4903\n",
      "Epoch 65/100\n",
      "3968/7740 [==============>...............] - ETA: 0s - loss: 0.4869"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10,\n",
    "cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=\n",
    "[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
