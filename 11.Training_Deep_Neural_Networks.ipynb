{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing relatively shallow nets, let's move on to deeper DNNs (layers >= 10; neurons/layer: X00, X000; connections: X0,000). \n",
    "\n",
    "Here are some problems we may encounter along the way，and some techniques we may try out to solve them:\n",
    "\n",
    "1. Vanishing / Exploding gradients problem making lower layers very hard to train \n",
    "                > Initialization \n",
    "2. Not enough data / too costly to label \n",
    "                > Transfer learning and unsupervised pretraining\n",
    "3. Painfully slow training \n",
    "                > Optimizers to the rescue!\n",
    "4. Serious overfitting risk for millions params models, especially if there are not enough training instances or if they are too noisy \n",
    "                > Good ol' (and new) regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vanishing / Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from our previous chapter, Gradient Descent goes from output > input layer propagating the error gradient along the way. Once it has computed the gradient of the cost function for each param of the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "**Vanishing** gradients gets smaller and smaller, leaving lower layers weights virtually unchanged (no convergence to good solution).  \n",
    "**Exploding** gradients gets bigger and bigger, making lower layers weights extremely large (divergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavious was not clearly understood until Glorot and Benjo suggested in a 2010 [paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) that this may be due to the the logistic sigmoid function and the weight initialization technique (normal dist 0,1).\n",
    "\n",
    "In short, they showed that with this activation function and this initialization scheme, the **each layer outputs variance > inputs variance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glorot and He Initialization\n",
    "\n",
    "The ideal solution would therefore be to have $var_{input} = var_{output}$ and $var_{forward} = var_{backwards}$. \n",
    "\n",
    "It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons ($fan_{in} = fan_{out}$) but there is workaround, i.e. initialize connection weight randomly from either:\n",
    "* Normal dist of mean $0$ and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "* Uniform dist between {-r,r} with $r = \\sqrt\\frac{3}{fan_{avg}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other initializations exist, differing in the variance used:\n",
    "\n",
    "**Initialization** | **Activation function** | **$\\sigma^2$(Normal)** \n",
    "-|-|-|\n",
    "Glorot | None, tanh, logistic, softmax | $\\frac{1}{fan_{avg}}$ \n",
    "He | ReLU and variants | $\\frac{2}{fan_{in}}$\n",
    "LeCun | SELU | $\\frac{1}{fan_{in}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonsaturating activation functions\n",
    "\n",
    "Altough the ReLU function solves some of the issues of the sigmoid function, it is far from perfect. A common issue are **dying ReLUs**, neurons which die when their weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set, and therefore keeps their gradients will keep outputting 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem, we could employ a **leaky ReLU**, which doesn't allow the neurons to die since it has a slope $\\alpha$ also when $z<0$.  \n",
    "\n",
    "![LeakyReLU](images/11.Leaky_ReLU.png)\n",
    "\n",
    "Slope is generally 0.01 but could also be:\n",
    "* Randomized: $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing (seems also to work as regularizer)\n",
    "* Parametric: $\\alpha$ authorized to be learned during training (not an hyperparam). Performs strongly on complex datasets but prone to overfitting in small ones. \n",
    "\n",
    "Finally, in 2015 Clevert et al. proposed a new activation function called the **Exponential Linear Unit (ELU)**.\n",
    "\n",
    "$ELU_{\\alpha}{(z)} = \n",
    "\\begin{cases}\n",
    "\\alpha(exp(z) - 1) & z < 0\\\\\n",
    "z & z \\ge 0\n",
    "\\end{cases}$\n",
    "\n",
    "![ELU](images/11.ELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantanges:\n",
    "\n",
    "* Negative values when $z<0$ which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem\n",
    "* Non-zero gradient for $z<0$, avoid dead neuron issue\n",
    "* If $\\alpha = 0$ function is smooth everywhere, including $z=0$ which helps speed up Gradient Descent \n",
    "\n",
    "**Note**: $\\alpha$ is the value that the ELU function approaches when $z$ is a large negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least **Scaled ELU (SELU)**, which under certain conditions outperforms all the above and self-normalize:\n",
    "\n",
    "* Standardized input features (0,1)\n",
    "* Hidden layers using LeCun normal initialization\n",
    "* Sequential network architecture (may not work on RNN for instance）\n",
    "* Dense layers (but may work with CNN as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Generally, SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic BUT most used (and optimized) is still **ReLU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although He + ELU can reduce problems at beginning of training, they could still come back later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
