{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing relatively shallow nets, let's move on to deeper DNNs (layers >= 10; neurons/layer: X00, X000; connections: X0,000). \n",
    "\n",
    "Here are some problems we may encounter along the way，and some techniques we may try out to solve them:\n",
    "\n",
    "1. Vanishing / Exploding gradients problem making lower layers very hard to train \n",
    "                > Initialization \n",
    "2. Not enough data / too costly to label \n",
    "                > Transfer learning and unsupervised pretraining\n",
    "3. Painfully slow training \n",
    "                > Optimizers to the rescue!\n",
    "4. Serious overfitting risk for millions params models, especially if there are not enough training instances or if they are too noisy \n",
    "                > Good ol' (and new) regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vanishing / Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from our previous chapter, Gradient Descent goes from output > input layer propagating the error gradient along the way. Once it has computed the gradient of the cost function for each param of the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "**Vanishing** gradients gets smaller and smaller, leaving lower layers weights virtually unchanged (no convergence to good solution).  \n",
    "**Exploding** gradients gets bigger and bigger, making lower layers weights extremely large (divergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavious was not clearly understood until Glorot and Benjo suggested in a 2010 [paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) that this may be due to the the logistic sigmoid function and the weight initialization technique (normal dist 0,1).\n",
    "\n",
    "In short, they showed that with this activation function and this initialization scheme, the **each layer outputs variance > inputs variance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glorot and He Initialization\n",
    "\n",
    "The ideal solution would therefore be to have $var_{input} = var_{output}$ and $var_{forward} = var_{backwards}$. \n",
    "\n",
    "It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons ($fan_{in} = fan_{out}$) but there is workaround, i.e. initialize connection weight randomly from either:\n",
    "* Normal dist of mean $0$ and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "* Uniform dist between {-r,r} with $r = \\sqrt\\frac{3}{fan_{avg}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other initializations exist, differing in the variance used:\n",
    "\n",
    "**Initialization** | **Activation function** | **$\\sigma^2$(Normal)** \n",
    "-|-|-|\n",
    "Glorot | None, tanh, logistic, softmax | $\\frac{1}{fan_{avg}}$ \n",
    "He | ReLU and variants | $\\frac{2}{fan_{in}}$\n",
    "LeCun | SELU | $\\frac{1}{fan_{in}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonsaturating activation functions\n",
    "\n",
    "Altough the ReLU function solves some of the issues of the sigmoid function, it is far from perfect. A common issue are **dying ReLUs**, neurons which die when their weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set, and therefore keeps their gradients will keep outputting 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem, we could employ a **leaky ReLU**, which doesn't allow the neurons to die since it has a slope $\\alpha$ also when $z<0$.  \n",
    "\n",
    "![LeakyReLU](images/11.Leaky_ReLU.png)\n",
    "\n",
    "Slope is generally 0.01 but could also be:\n",
    "* Randomized: $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing (seems also to work as regularizer)\n",
    "* Parametric: $\\alpha$ authorized to be learned during training (not an hyperparam). Performs strongly on complex datasets but prone to overfitting in small ones. \n",
    "\n",
    "Finally, in 2015 Clevert et al. proposed a new activation function called the **Exponential Linear Unit (ELU)**.\n",
    "\n",
    "$ELU_{\\alpha}{(z)} = \n",
    "\\begin{cases}\n",
    "\\alpha(exp(z) - 1) & z < 0\\\\\n",
    "z & z \\ge 0\n",
    "\\end{cases}$\n",
    "\n",
    "![ELU](images/11.ELU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantanges:\n",
    "\n",
    "* Negative values when $z<0$ which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem\n",
    "* Non-zero gradient for $z<0$, avoid dead neuron issue\n",
    "* If $\\alpha = 0$ function is smooth everywhere, including $z=0$ which helps speed up Gradient Descent \n",
    "\n",
    "**Note**: $\\alpha$ is the value that the ELU function approaches when $z$ is a large negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least **Scaled ELU (SELU)**, which under certain conditions outperforms all the above and self-normalize:\n",
    "\n",
    "* Standardized input features (0,1)\n",
    "* Hidden layers using LeCun normal initialization\n",
    "* Sequential network architecture (may not work on RNN for instance）\n",
    "* Dense layers (but may work with CNN as well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip**: Generally, SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic BUT most used (and optimized) is still **ReLU**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although He + ELU can reduce problems at beginning of training, they could still come back later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be addressed with **Batch Normalization**. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer. The operation lets the model learn the optimal _scale_ and _mean_ of each of the layer’s inputs.\n",
    "\n",
    "In many cases, a BN layer can act as stardization for the training set. \n",
    "\n",
    "Step by step (in non-mathematical notation):\n",
    "\n",
    "1. $\\mu_B$ = Mean\n",
    "2. $\\sigma_B$ = Standard deviation\n",
    "3. $\\hat{x}^{(i)}$ = Vector of zero-centered and normalized inputs for instance $i$\n",
    "4. $z^{(i)} = \\gamma \\otimes \\hat{x}^{(i)} + \\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, the model is learning:\n",
    "\n",
    "* $\\gamma$ = output scale vector (through backprop)\n",
    "* $\\beta$ = output offset vector (through backprop)\n",
    "* $\\mu$ = final input mean vector (through exponential moving avg)\n",
    "* $\\sigma$ = final input stdev vector (through exponential moving avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: generally speaking, BN will make make epoch slower but will require less epochs. All in all, generally it saves time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional hyperparameter we may have to pay attention to is **momentum**, used in the calculation of running average as such:\n",
    "\n",
    "$\\hat{v}_1 = \\hat{v}_0 \\times momentum + v \\times (1 - momentum)$\n",
    "\n",
    "Generally a good value is very close to 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Clipping\n",
    "\n",
    "Another technique involves clipping the gradients so that they never exceed a certain threshold. Mostly used for RNNs, as BN is harder to apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reusing Pretrained Layers\n",
    "\n",
    "**General note**: this section will have no cell output since we are working with fictional `model_A` and `model_B`.\n",
    "\n",
    "Generally we don't want to train DNN from scratch. We want to find a network that may accomplish what we want to do and reuse the lower layers (**transfer learning**).\n",
    "The most similar the tasks, the most layers we can reuse. We can check this by progressively _freezing_ upper layers and leaving their weights fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transfer Learning with Keras\n",
    "\n",
    "Let's use our Fashion MNIST dataset. We have a model A that works well (90% accuracy) on 8 classes excluding sandals and shirts, and want to train a new binary classifier (shirt/sandals) and apply it to only 200 images (model B).\n",
    "\n",
    "Why not reuse model A? Except for the output layer, of course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second step `model_B_on_A` will also affect model A. If we want to avoid that, we need to clone it first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the new output layer was initialized randomly it will make large errors (at least at the beginning) so a common solution is to **freeze the reused layers** during the first epochs, giving the new layer some time to learn reasonable weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",\n",
    "metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: always compile model after freezing or unfreezing layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After unfreezing the reused layers, it is usually a good idea to **reduce the learning rate**, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                            validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                    metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                            validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach may lead to vast improvements, but are we being 'honest' here? Probably not. What we are doing is simply trying different configurations until we find one that yields a strong improvement. \n",
    "\n",
    "So why we had to do this in the first place? It turns out that transfer learning does **not** work very well with small dense networks, presumably because small networks learn few\n",
    "patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Pretraining\n",
    "\n",
    "If we want to tackle complex task without much labeled data and similar model, we can still perform _unsupervised pretraining_. \n",
    "\n",
    "Basically, we can train an autoencoder or GAN for the lower layers, add the output layer for our task on top, and fine-tune the final network using supervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pretraining on an Auxiliary Task\n",
    "\n",
    "Another option is to train a first NN on a similar (auxiliary) task for which labeled data is more available and then reuse the lower layers for the actual task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Faster Optimizers\n",
    "\n",
    "So far we have seen four ways to speed up NN training:\n",
    "\n",
    "1. Good initialization strategy for connecting weights\n",
    "2. Good activation function\n",
    "3. Batch Normalization\n",
    "4. Reusing parts of a pretrained network\n",
    "\n",
    "In this section we will cover an additional set of tools: **faster optimizers** (than our plain vanilla Gradient Descent). More specifically:\n",
    "\n",
    "1. Momentum Optimization \n",
    "2. Nesterov Accelerated Gradient\n",
    "3. AdaGrad\n",
    "4. RMSProp\n",
    "5. Adam and Nadam optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Momentum Optimization\n",
    "\n",
    "The intuition behind momentum optimization (and hence the name) is physical momentum.\n",
    "\n",
    "Our Gradient Descent does not care about what the earlier gradients were, since it simply updates the last weights: $\\theta \\leftarrow \\theta - \\eta \\triangledown_\\theta J(\\theta)$. \n",
    "Momentum Optimization, on the other hand, at each iteration subtracts the local gradients from the **momentum vector** and it updates the weights by adding this momentum vector. Basically, we are looking at acceleration rather than speed. \n",
    "The tecnique also introduce a parameter $\\beta$ (**friction**) to deal with excessive \"acceleration\" and help with convergence. \n",
    "\n",
    "More formally:\n",
    "\n",
    "1. $m \\leftarrow \\beta m - \\eta \\triangledown_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta + m$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super-easy Keras implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Nesterov Accelerated Gradient\n",
    "\n",
    "Almost always faster than vanilla momentum optimization. NAG measures the gradient of the cost function not at the local position $\\theta$ but slightly ahead in the direction of the momentum, at $\\theta + \\beta m$:\n",
    "\n",
    "1. $m \\leftarrow \\beta m - \\eta \\triangledown_\\theta J(\\theta + \\beta m)$\n",
    "2. $\\theta \\leftarrow \\theta + m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. AdaGrad\n",
    "\n",
    "AdaGrad correct the direction of the algorithm erlier to facilitate convergence. It does this by scaling down the gradient vector along the steepest dimensions:\n",
    "\n",
    "1. $s \\leftarrow s - \\triangledown_\\theta J(\\theta) \\otimes \\triangledown_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta \\triangledown_\\theta J(\\theta) \\oslash \\sqrt{s+\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: AdaGrad is not a good choice for training NNs. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. RMSProp\n",
    "\n",
    "RMSProp fixes the \"slowing down too fast\" issue from AdaGrad by accumulating only the gradients from the most recent iterations:\n",
    "\n",
    "1. $s \\leftarrow \\beta s + (1- \\beta) \\triangledown_\\theta J(\\theta) \\otimes \\triangledown_\\theta J(\\theta)$\n",
    "2. $\\theta \\leftarrow \\theta - \\eta \\triangledown_\\theta J(\\theta) \\oslash \\sqrt{s+\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decay rate is typically set to 0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Adam and Nadam Optimization\n",
    "\n",
    "Adam (adaptive moment estimation) combines RMSProp with Momentum Optimization: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and\n",
    "just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients:\n",
    "\n",
    "1. $m \\leftarrow \\beta_1 m - (1-\\beta_1) \\triangledown_\\theta J(\\theta)$\n",
    "2. $s \\leftarrow \\beta_2 s + (1- \\beta_2) \\triangledown_\\theta J(\\theta) \\otimes \\triangledown_\\theta J(\\theta)$\n",
    "3. $\\hat{m} \\leftarrow \\frac{m}{1-\\beta_1^T}$\n",
    "4. $\\hat{s} \\leftarrow \\frac{s}{1-\\beta_2^T}$\n",
    "5. $\\theta \\leftarrow \\theta + \\eta hat{m} \\oslash \\sqrt{\\hat{s}+\\epsilon}$\n",
    "\n",
    "**Note 1**: 3 and 4 help boost $s$ and $m$ at beginning of training.   \n",
    "**Note 2**: usually $b_1$ = 0.9 | $b_2$ = 0.999 | $\\epsilon$ = $10^{-7}$ | $\\eta$ = 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variant is Nadam (Adam + NAG) to converge faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on optimization\n",
    "\n",
    "1. Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, but **may generalize poorly**. If model performance is not satisfactory, turn to NAG.\n",
    "2. Everything discussed above is based on **first-order partial derivatives** (Jacobians). In literature we may also find **second-order partial derivatives** (Hessian) but given that there are $n^2$ (n = # params) Hessians per output this makes them less practical for DNNs with X0,000 params.\n",
    "3. Everything above will lead to dense models. If you want to work with sparse models, you can:\n",
    "    * Get rid of tiny weights\n",
    "    * Apply strong $l_1$ regularization as it pushes the optimizer to zero out as many weights as it can"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizers comparison table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Optimizers](images\\11.Optimizers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduling\n",
    "\n",
    "**1. Power scheduling**  \n",
    "Set the learning rate to a function of the iteration number: $\\displaystyle t: \\eta(t) = \\frac{\\eta_0}{(1+\\frac{t}{s})^c}$\n",
    "\n",
    "**2. Exponential scheduling**  \n",
    "Set learning rate to $\\displaystyle \\eta(t) = \\eta_0 0.1^{t/s}$\n",
    "\n",
    "**3. Piecewise constant scheduling**  \n",
    "Constant learning rate for a number of epochs. The dirty secret is then: what is the right sequence and for how long?\n",
    "\n",
    "**4. Performance scheduling**    \n",
    "Measure validation error every $N$ steps and reduce learning rate by $\\gamma$ factor every time the error stops dropping.\n",
    "\n",
    "**5. 1cycle scheduling**  \n",
    "1cycle starts by increasing the initial learning rate $\\eta_0$ to $\\eta_1$ around halfway through training. Then dicrease linearly again to $\\eta_0$ during the second half of training. \n",
    "$\\eta_0$ = optimal learning rate\n",
    "$\\eta_1$ = 10 times lower than $\\eta_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Avoiding Overfitting Through Regularization\n",
    "\n",
    "_\"With many parameters come great regularization\"_ or something like that.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\ell_1$ and $\\ell_2$ Regularization\n",
    "\n",
    "Implementation using a regularization factor of 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "                           \n",
    "kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`keras.regularizers.l1()` and `keras.regularizers.l1_l2()` are also available. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid rewriting regularizers, activation function and initialization strategy for all hidden layers, we can use `functools.partial()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-105a890b18a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m RegularizedDense = partial(keras.layers.Dense,\n\u001b[0m\u001b[0;32m      4\u001b[0m                             \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"elu\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                             \u001b[0mkernel_initializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"he_normal\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                            activation=\"elu\",\n",
    "                            kernel_initializer=\"he_normal\",\n",
    "\n",
    "kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                    kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "One of the most popular regularization techniques for DL. It is extremely effective (1-2% accuracy boost) and fairly simple: for every training step, every neuron (including inputs) has a % $p$ (or _dropout rate_, on avg. 10%-50% and 20-30% for RNNs and 40-50% for CNNs) of being temporarily dropped. \n",
    "\n",
    "To a certain extend, we can see this as a way to training a neural network which is a an ensemble of smaller NNs trained on one training instance (since basically all NNs, will be different from each other, albeit very similar).\n",
    "\n",
    "**Note 1**: In practice we only apply dropout to last one to three layers (excl output).  \n",
    "**Note 2**: We need to multiply each input connection weight by the keep probability (1 – p) after training.  \n",
    "**Note 3**: Evaluate the training loss without dropout (e.g., after training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-44b3d7453b17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = keras.models.Sequential([\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m28\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     keras.layers.Dense(300, activation=\"elu\",\n\u001b[0;32m      5\u001b[0m kernel_initializer=\"he_normal\"),\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking:\n",
    "\n",
    "* Model overfitting > Increase dropout rate\n",
    "* Model underfitting > Decrease dropout rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
