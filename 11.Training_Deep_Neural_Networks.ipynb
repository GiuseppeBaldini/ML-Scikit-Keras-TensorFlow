{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Training Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After introducing relatively shallow nets, let's move on to deeper DNNs (layers >= 10; neurons/layer: X00, X000; connections: X0,000). \n",
    "\n",
    "Here are some problems we may encounter along the wayï¼Œand some techniques we may try out to solve them:\n",
    "\n",
    "1. Vanishing / Exploding gradients problem making lower layers very hard to train \n",
    "                > Initialization \n",
    "2. Not enough data / too costly to label \n",
    "                > Transfer learning and unsupervised pretraining\n",
    "3. Painfully slow training \n",
    "                > Optimizers to the rescue!\n",
    "4. Serious overfitting risk for millions params models, especially if there are not enough training instances or if they are too noisy \n",
    "                > Good ol' (and new) regularization techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vanishing / Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from our previous chapter, Gradient Descent goes from output > input layer propagating the error gradient along the way. Once it has computed the gradient of the cost function for each param of the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "\n",
    "**Vanishing** gradients gets smaller and smaller, leaving lower layers weights virtually unchanged (no convergence to good solution).  \n",
    "**Exploding** gradients gets bigger and bigger, making lower layers weights extremely large (divergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This behavious was not clearly understood until Glorot and Benjo suggested in a 2010 [paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) that this may be due to the the logistic sigmoid function and the weight initialization technique (normal dist 0,1).\n",
    "\n",
    "In short, they showed that with this activation function and this initialization scheme, the **each layer outputs variance > inputs variance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glorot and He Initialization\n",
    "\n",
    "The ideal solution would therefore be to have $var_{input} = var_{output}$ and $var_{forward} = var_{backwards}$. \n",
    "\n",
    "It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons ($fan_{in} = fan_{out}$) but there is workaround, i.e. initialize connection weight randomly from either:\n",
    "* Normal dist of mean $0$ and variance $\\sigma^2 = \\frac{1}{fan_{avg}}$\n",
    "* Uniform dist between {-r,r} with $r = \\sqrt\\frac{3}{fan_{avg}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other initializations exist, differing in the variance used:\n",
    "\n",
    "**Initialization** | **Activation function** | **$\\sigma^2$(Normal)** \n",
    "-|-|-|\n",
    "Glorot | None, tanh, logistic, softmax | $\\frac{1}{fan_{avg}}$ \n",
    "He | ReLU and variants | $\\frac{2}{fan_{in}}$\n",
    "LeCun | SELU | $\\frac{1}{fan_{in}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonsaturating activation functions\n",
    "\n",
    "Altough the ReLU function solves some of the issues of the sigmoid function, it is far from perfect. A common issue are **dying ReLUs**, neurons which die when their weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set, and therefore keeps their gradients will keep outputting 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem, we could employ a **leaky ReLU**, which doesn't allow the neurons to die since it has a slope $\\alpha$ also when $z<0$.  \n",
    "Slope is generally 0.01 but could also be:\n",
    "* Randomized: $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing (seems also to work as regularizer)\n",
    "* Parametric: $\\alpha$ authorized to be learned during training (not an hyperparam). Performs strongly on complex datasets but prone to overfitting in small ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
