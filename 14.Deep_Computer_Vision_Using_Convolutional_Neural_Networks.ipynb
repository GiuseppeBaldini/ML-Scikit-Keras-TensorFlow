{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Deep Computer Vision Using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual cortex, and they have been used in image recognition since the 1980s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layers\n",
    "\n",
    "Neurons in the first convolutional layer are **not** connected to every pixel in the input image but only to pixels in their receptive fields. Successive layers only concentrate on a rectangle of neurons in the previous layers. \n",
    "\n",
    "**Note**: in CNNs each layer is in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neuron located in row $i$, column $j$ of a given layer is connected to the outputs of the neurons in the previous layer located in rows $i$ to $i + f_h – 1$, columns $j$ to $j + f_w – 1$, where $f_h$ and $f_w$ are the height and width of the receptive field. In order to have same height and weight for layers, zeros are added around inputs (**Zero padding**).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN_Layers](images/13.CNN_Layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shift from one receptive field to the next is called the **stride**. The ouput layer can be smaller than the input layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filters\n",
    "\n",
    "Or **convolutional kernels** can be represented as a small image the size of the receptive field. These weights will put particular emphasis to certain features of the data (hence the names _feature map_ for their otput), e.g. horizontal lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stacking Multiple Feature Maps\n",
    "\n",
    "In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow Implementation\n",
    "\n",
    "Input image = 3D tensor [_height, width, channels_]  \n",
    "Mini-batch = 4D tensor [_mini-batch size, height, width, channels_]  \n",
    "CNN weights = 4D tensor [$f_h$, $f_w$, $f_{n'}$, $f_n$]  \n",
    "Bias term = 1D tensor [$f_n$]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "The goal of pooling layers is **subsample** the input image in order to reduce the computational load, the memory usage, and the number of parameters. A pooling neuron has no weights, all it does is **aggregate inputs** using an aggregator function such as max or mean. \n",
    "\n",
    "#### TensorFlow Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# using max\n",
    "max_pool = keras.layers.MaxPool2D(pool_size=2)\n",
    "# using average\n",
    "avg_pool = keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, mean pooling is more popular than average pooling probable because it maintains only the strongest feature, eliminating potential noise. Also, it takes less to compute and offers stronger translation invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architectures\n",
    "\n",
    "Typical CNN architectures stack a few convolutional layers (each one generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. Images gets **smaller and deeper** as they progress through the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\",\n",
    "                        input_shape=[28, 28, 1]),\n",
    "    # max pooling layer (pooling size 2)\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    # doubling n of filers in after each polling layer\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    # flatten input into 1D array\n",
    "    keras.layers.Flatten(),\n",
    "    # fully connected network\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pretrained Models from Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load pretrained networks very easily in Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 2cb95161c43110f7111970584f804107 so we will re-download the data.\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102973440/102967424 [==============================] - 69s 1us/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Models for Transfer Learning\n",
    "\n",
    "If we want to build an image classifier but we do not have enough training data, it is often a good idea to reuse the lower layers of a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True,\n",
    "with_info=True)\n",
    "dataset_size = info.splits[\"train\"].num_examples # 3670\n",
    "class_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\n",
    "n_classes = info.features[\"label\"].num_classes # 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we will need to do the splitting ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\n",
    "valid_set = tfds.load(\"tf_flowers\", split=valid_split,\n",
    "as_supervised=True)\n",
    "train_set = tfds.load(\"tf_flowers\", split=train_split,\n",
    "as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing (our CNN expects 224 x 224):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s apply this preprocessing function to all three datasets, shuffle the training set, and add batching and prefetching to all the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 32\n",
    "train_set = train_set.shuffle(1000)\n",
    "train_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the Xception model, pretrained on ImageNet, excluding top of network. Then we then add our own global average pooling layer, based on the output of the base model, followed by a dense output layer with one unit per class, using the softmax activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 76s 1us/step\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "            include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s usually a good idea to freeze the weights of the pretrained layers, at least at the beginning of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compile the model and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 958s 11s/step - loss: 0.7640 - accuracy: 0.7632 - val_loss: 1.8927 - val_accuracy: 0.7495\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 909s 11s/step - loss: 0.3995 - accuracy: 0.8776 - val_loss: 1.2836 - val_accuracy: 0.7514\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 1243s 14s/step - loss: 0.2512 - accuracy: 0.9075 - val_loss: 1.0065 - val_accuracy: 0.7964\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 1414s 16s/step - loss: 0.2240 - accuracy: 0.9231 - val_loss: 0.8289 - val_accuracy: 0.8144\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 1410s 16s/step - loss: 0.1840 - accuracy: 0.9392 - val_loss: 0.8517 - val_accuracy: 0.8126\n"
     ]
    }
   ],
   "source": [
    "# WARNING：this could take a while without a GPU\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer=optimizer,\n",
    "        metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After top layers have been trained, we are ready to unfreeze all the layers (or just the top ones) and continue training. Let's not forget to **compile** the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 92150s 1072s/step - loss: 0.2239 - accuracy: 0.9246 - val_loss: 0.6019 - val_accuracy: 0.8685\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 52546s 611s/step - loss: 0.0611 - accuracy: 0.9829 - val_loss: 0.3631 - val_accuracy: 0.9081\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 1782s 21s/step - loss: 0.0180 - accuracy: 0.9931 - val_loss: 0.2664 - val_accuracy: 0.9333\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 1543s 18s/step - loss: 0.0153 - accuracy: 0.9960 - val_loss: 0.2504 - val_accuracy: 0.9315\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 1225s 14s/step - loss: 0.0097 - accuracy: 0.9967 - val_loss: 0.2651 - val_accuracy: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# WARNING: this also may take a while \n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "    \n",
    "# lower learning rate and decay\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "             optimizer=optimizer,\n",
    "             metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is more to computer vision than just classification. After knowing _what_ things are, how about knowing _where_ are they? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Localization\n",
    "\n",
    "A bouding box around an object can be predicted using four coordinates: horizontal and vertical coordinates of the center + height and width. \n",
    "\n",
    "This means we can accomplish this by adding a second dense output layer with four units (typically on top of the global average pooling layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-0c52cbee00e7>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-14-0c52cbee00e7>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    optimizer=optimizer, metrics=[\"accuracy\"])\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = keras.layers.Dense(4)(avg)\n",
    "model = keras.Model(inputs=base_model.input,\n",
    "                    outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "            loss_weights=[0.8, 0.2], # depends on what you care most\n",
    "about\n",
    "            optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evaluation metric, it is commonplace to use **Intersection over Union**, which as the name implies is （area of intersection between predicted box and actual box) / (area union). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection\n",
    "\n",
    "Previously, object detection was done primarily by sliding rectangles of varying sizes to the image, followed by getting rid of unnecessary boxes. Usually this is done by:\n",
    "\n",
    "1. Selecting all boxes where our _objectness score_ is higher than a certain threshold\n",
    "2. Finding the bounding box with the highest objectness score, and getting rid of all the other bounding boxes that overlap a lot with it\n",
    "3. Repeat until there is only one box left\n",
    "\n",
    "Since 2015 there is a new guy in town. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mean Average Precision (mAP)\n",
    "\n",
    "Very common object detection metric. Suppose we have a classfier with 90% precision at 10% recall, and 96% precision at 20% recall. Obviously the second one is superior to the first. So what we should be looking at is the **maximum** precision model that satisfies **at least** a minimum recall threshold.\n",
    "\n",
    "What we do is to calculate the precision at different levels of recall and average it. Pretty straightforward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation\n",
    "\n",
    "In semantic segmentation, every pixel is classified according to the class of the object it belongs to. Pixel of the same class all end up together. \n",
    "\n",
    "As for object detection, there are many approaches. A fairly simple one was suggested by Jonathan Long et al. in 2015. In short:\n",
    "\n",
    "* Take pretrained CNN and turn it into a FCN\n",
    "* The CNN applies an overall stride of 32 to the input image (so last layer output feature maps are 32 times smaller than original)\n",
    "* Add unsampling layer to get to full resolution back. Several approaches possible here:\n",
    "    * Transposed convolutional layer (first stretching the image by inserting empty rows and columns (full of zeros), then performing a regular convolution\n",
    "    * Regular convolutional layer that uses fractional strides\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
