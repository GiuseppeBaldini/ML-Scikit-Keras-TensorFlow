{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most applications today are in supervised learning, most of the data available is actually unlabeled. \n",
    "\n",
    "Here is where unsupervised learning shines. In this chapter, we will look at three unsupervised learning tasks:\n",
    "\n",
    "1. **Clustering**: group similar instances in classes\n",
    "2. **Anomaly detection**: learn what is normal data to detect abnormal instances\n",
    "3. **Density estimation**: estimating the probability density function (PDF) of the random process that generated the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Clustering\n",
    "\n",
    "Examples of clustering algorithms include:* \n",
    "\n",
    "* Segmentation\n",
    "* Data analysis\n",
    "* Dimensionality reduction\n",
    "* Anomaly detection\n",
    "* Semi-supervised learning\n",
    "* Search engines\n",
    "* Image compression\n",
    "\n",
    "Let's now look at two particular algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means\n",
    "\n",
    "K-means is a relatively simple yet powerful algorithm that will try to find each cluster’s center and assign each instance to the closest cluster.\n",
    "\n",
    "Let's try it out on built-in blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of clustering, an instance’s label is the index of the cluster that this instance gets assigned to by the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 2, ..., 3, 2, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred is kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also have a look at the centroids:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use them to quickly assign new instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of assigning each instance to a single cluster (**hard clustering**), it can be useful to just give each instance a score per cluster (**soft clustering**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means algorithm\n",
    "\n",
    "The algorithm works by initially placing clusters in a random position and iterating until convergence, which usually happens in few steps and linear computational complexity with regards to the number of instances _m_, the number of clusters _k_ and the number of dimensions _n_.\n",
    "\n",
    "However, guarantee of convergence is not guarantee of global optimum. Improving centroid initialization can therefore lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do this by:\n",
    "\n",
    "1. Setting the centroids ourselves, usually after running a first random init iteration\n",
    "2. Run the algorithm multiple times with different random initializations and keep the best solution (the one with minimal _inertia_ - mean squared distance between each instance and its closest centroid)\n",
    "3. Use the K-means +/+ implementation which works by selecting centroids that are distance from one another\n",
    "\n",
    "The last option, developed by David Arthur and Sergei Vassilvitskii in 2006, is the default Scikit-learn implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerated K-means and Mini-batch K-means \n",
    "\n",
    "Another improvement to the algorithm was proposed by Charles Elkan in 2003 and take advantage of the triangle inequality ($AC ≤ AB + BC$) to reduce computation of the distances. This approach is used in the default implementation of Scikit-learn. \n",
    "\n",
    "To reduce storage requirements, in 2010 David Sculley proposed using mini-batches, speeding things up by 3-4 times.   \n",
    "Although the Mini-batch K-Means algorithm is much faster than the regular K-Means algorithm, its inertia is generally slightly worse, especially as $k$ increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal numbers of clusters\n",
    "\n",
    "Unfortunately, inertia is not a good criteria for choosing $k$, so let's see a few others:\n",
    "\n",
    "1. A common approach is the **elbow rule**, which stems from the similarity of the inertia-k graph to a human arm. In this case, we would choose the $k$ which lies on the steepest change of slope (the _elbow_ of our _arm_)\n",
    "\n",
    "2. A more precise and computationally expensive method is to use the **silhouette score** (mean of the silhouette coefficient over all the instances). The coefficient varies from -1 to +1, where:\n",
    "* -1 = instance may have been assigned to the wrong cluster\n",
    "* 0 = instance close to cluster boundary\n",
    "* +1 = instance well inside cluster boundaries and far from other clusters\n",
    "\n",
    "**Note:** The silhouette coefficient is equal to: $\\frac{(b-a)}{max(a,b)}$ where \n",
    "$a$ = mean distanceto the other instances in the same cluster\n",
    "$b$ = mean distance to the instances of the next closest cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limits of K-means\n",
    "\n",
    "Despite its advantages (fast - scalable) K-means encounters strong limitations when it comes to clusters of varying sizes, densities, or non-spherical shapes. In addition, it may lead to suboptimal solutions due to initialization. \n",
    "\n",
    "**Note**: Scaling the inputs may help, but does not guarantee optimal results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application: Preprocessing\n",
    "\n",
    "Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing step before a supervised learning algorithm.\n",
    "\n",
    "For example, using MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "X_digits, y_digits = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting in training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate accuracy\n",
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "95.5% baseline, not bad. Let’s see if we can do better by using K-Means as a preprocessing step. We will create a pipeline to cluster the training set in 50 clusters > replace the images with their distances to these 50 clusters > train our logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('kmeans',\n",
       "                 KMeans(algorithm='auto', copy_x=True, init='k-means++',\n",
       "                        max_iter=300, n_clusters=50, n_init=10, n_jobs=None,\n",
       "                        precompute_distances='auto', random_state=None,\n",
       "                        tol=0.0001, verbose=0)),\n",
       "                ('log_reg',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "(\"kmeans\", KMeans(n_clusters=50)),\n",
    "(\"log_reg\", LogisticRegression()),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already basically halved our error, and we did so by choosing $k$ arbitrarily. Let’s use `GridSearchCV` to find the optimal number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# suppressing output \n",
    "%%capture\n",
    "param_grid = dict(kmeans__n_clusters=range(2, 100))\n",
    "grid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\n",
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kmeans__n_clusters': 57}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best value of k\n",
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9777777777777777"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final accuracy: 97.7% "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application: Semi-Supervised learning\n",
    "\n",
    "Another useful application of clustering is semi-supervised learning, when we have plenty of unlabeled instances and very few labeled instances. For example, let's train a logistic regression on 50 labeled instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_labeled = 50\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8155555555555556"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great, as we would expect. Let's now cluster the training set in 50 clusters, then find the image closest to the centroid (**representative images**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these representative images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 50 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "for index, X_representative_digit in enumerate(X_representative_digits):\n",
    "    plt.subplot(k // 10, 10, index + 1)\n",
    "    plt.imshow(X_representative_digit.reshape(8, 8), cmap=\"binary\", interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And manually label them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_representative_digits = np.array([3, 6, 7, 0, 2, 5, 8, 4, 8, 5, \n",
    "                                    3, 1, 6, 7, 1, 7, 0, 1, 3, 9,\n",
    "                                    7, 9, 8, 5, 2, 7, 4, 1, 2, 4,\n",
    "                                    3, 4, 2, 8, 6, 5, 3, 9, 9, 8, \n",
    "                                    5, 0, 2, 9, 8, 7, 4, 7, 7, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if our overall performance increased:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_representative_digits, y_representative_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Almost a 10% improvement. Let's go further by extending our labels to all the instances in the same cluster (**label propagation**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train_propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955555555555555"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy dropped, why? Most likely this is due to the instances at the margins. Let’s see what happens if we only propagate the labels to the 5% of the instances that are closest to the centroids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_closest = 5\n",
    "\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "    \n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\giuse\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8977777777777778"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A slight improvement due to the very high accuracy of the partially propagated labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.967391304347826"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_train_partially_propagated == y_train[partially_propagated])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Active learning\n",
    "\n",
    "In uncertain situations, little human help can greatly improve our results. This is called active learning, and its most common strategy **uncertainty sampling**: \n",
    "\n",
    "1. Model is trained on the labeled instances gathered so far, and this model is used to make predictions on all the unlabeled instances\n",
    "2. Instances for which the model is most uncertain (i.e., when its estimated probability is lowest) must be labeled by the human expert\n",
    "3. Iterate this process again and again, until the performance improvement stops being worth the labeling effort\n",
    "\n",
    "Other strategies may label instances that cause:\n",
    "\n",
    "* Largest model change\n",
    "* Largest drop in validation error\n",
    "* Increase in _agreement_ among different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm defines clusters as continuous regions of high density by:\n",
    "\n",
    "1. Counting how many instances are located within a small distance $\\epsilon$ from it ($\\epsilon$-neighbourhood)\n",
    "2. If an instance has at least min_samples instances in its ε-neighborhood (including itself), then it is considered a **core instance**. In other words, core instances are those that are located in **dense regions**\n",
    "3. All instances in the neighborhood of a core instance belong to the same cluster. This may include other core instances, therefore a long sequence of neighboring core instances forms a single cluster.\n",
    "4. Any instance that is not a core instance and does not have one in its neighborhood is considered an **anomaly**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works on the moon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBSCAN(algorithm='auto', eps=0.05, leaf_size=30, metric='euclidean',\n",
       "       metric_params=None, min_samples=5, n_jobs=None, p=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.05)\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  0,  1, -1, -1,  2,  1,  2,  1,  3,  3,  3,  1,  4,  3,  1,  3,\n",
       "        1, -1,  0,  2,  3,  2,  0,  1, -1,  3,  3, -1, -1,  2,  0,  3,  0,\n",
       "        3,  2,  3,  1,  2,  3,  5,  1,  3,  3,  1,  1,  0,  1,  1,  1,  1,\n",
       "        0,  1,  3,  3, -1, -1,  5,  3,  3, -1,  1,  1,  6,  1,  3,  0,  3,\n",
       "        0,  1,  3,  6,  0,  3,  1,  3,  3,  1,  2,  2,  0,  0,  0,  1,  3,\n",
       "        1,  0,  3,  3,  1,  1,  1,  1,  1,  1,  0,  2,  1,  2, -1,  3,  1,\n",
       "        2,  0,  2,  5, -1, -1, -1,  0,  1,  3,  2,  3,  6,  0,  1,  3,  1,\n",
       "        4,  3,  3,  1,  2,  1,  0,  0,  3,  2, -1,  0,  1,  2,  1,  0,  2,\n",
       "        1,  3,  3,  0,  2,  3,  2,  2,  2,  1,  0,  3,  2,  0,  3,  1,  1,\n",
       "        1, -1,  1,  3,  0,  3,  3,  1,  1,  2,  2,  1,  3,  0,  3,  4,  1,\n",
       "        3,  1,  3,  2,  2,  3,  1,  1,  2,  4,  1, -1,  3,  0,  0,  0,  3,\n",
       "        0,  0,  3,  1,  0,  3,  2,  0,  0, -1,  3,  3,  1,  3,  1,  3,  3,\n",
       "        1,  5,  2,  3,  1, -1,  1,  0,  3,  3,  3,  3,  1, -1,  2,  2,  0,\n",
       "        3,  3,  1, -1,  3,  1,  0,  2,  1,  0,  1,  3,  1,  1,  1,  3,  2,\n",
       "        1,  3,  2, -1,  4,  3, -1,  3,  1,  2,  0,  3,  1,  2,  2,  3,  3,\n",
       "        3, -1,  2,  1,  2,  3,  1,  0, -1,  3,  1,  2,  0,  1,  3, -1,  2,\n",
       "        1,  3,  3,  3,  2,  2,  3,  3,  1,  3,  0,  0,  0,  3,  3,  1,  1,\n",
       "        3,  1,  0,  0,  2,  1,  0,  2,  3,  2, -1, -1,  3,  3,  3,  6,  1,\n",
       "        1,  1,  1,  0,  1,  0,  3,  3,  1, -1,  1,  5,  2,  2,  1,  0,  3,\n",
       "        1,  1,  6,  0,  3,  3,  3,  0,  0,  4,  1,  2,  3,  0,  3,  1,  2,\n",
       "        0,  2,  0,  0,  4,  1,  3,  1,  1,  1,  3,  1,  0,  2,  3,  1,  1,\n",
       "        1,  0,  1,  1,  1, -1,  1,  1,  1,  2,  3, -1,  3,  1,  1, -1,  1,\n",
       "        1,  1,  0,  0,  3,  1,  0,  2,  0,  3, -1,  3,  1,  3,  0,  3,  3,\n",
       "        0,  3,  0,  1,  1,  2,  3,  1,  2,  1,  2,  3,  1,  1,  2,  2,  3,\n",
       "        1,  0,  1,  3,  0,  3,  3,  0,  0,  2,  3,  6,  1,  3,  3,  2,  6,\n",
       "        1,  0,  1,  3,  4,  1, -1,  3,  2,  0,  2,  3,  2,  2,  1,  3,  3,\n",
       "        0,  3,  2,  1,  0,  2,  2,  1,  0,  1,  2, -1,  2,  3,  0,  6,  2,\n",
       "        3,  1,  0,  1,  1,  4,  3,  2,  3,  1,  4,  1,  3,  2,  1,  3,  2,\n",
       "        2,  2,  3,  3,  1,  1,  1, -1,  1,  0,  1,  0, -1,  0,  1,  1,  1,\n",
       "        2,  1,  0,  2,  1,  0,  0, -1,  0,  1,  2,  1,  1,  0,  0,  6,  0,\n",
       "        1,  0,  1,  1, -1,  3, -1,  1,  2,  0,  3,  6,  0,  1,  3,  1,  2,\n",
       "        0,  2,  0,  0,  1,  3,  0,  5,  1,  3,  0,  0,  3,  2,  2,  1,  1,\n",
       "        3,  3,  2,  2,  1,  1,  0,  2, -1,  1,  3,  3,  1,  3,  3,  0,  2,\n",
       "        1,  3,  0,  3,  3,  2, -1, -1,  1,  2,  2,  1,  1,  1,  1,  2,  0,\n",
       "        1,  1,  5,  1,  1,  1,  3,  3,  1,  0,  4,  2,  1,  2,  2,  1,  1,\n",
       "       -1,  2,  1,  3,  4,  1,  1,  1,  2,  3,  1,  0,  0,  1,  1,  3,  2,\n",
       "        0,  3,  3,  1,  1,  3,  2,  1,  1,  1, -1,  1,  3,  3,  1,  1,  0,\n",
       "        6,  1,  3,  3,  1,  2,  2,  3,  1,  3,  3,  1,  1,  2,  1,  3,  0,\n",
       "        1,  1,  3,  1,  0,  0,  1,  2,  3,  1,  1,  3,  2,  0,  0, -1,  3,\n",
       "        3,  3,  1,  1,  0,  3,  3,  3,  1,  0,  0,  3,  1,  1,  1,  1,  2,\n",
       "        1,  3,  3,  1,  0,  1,  0,  3,  2,  3,  1,  3,  2,  1,  2,  1,  2,\n",
       "        3,  1,  6,  0,  2,  3,  3,  1,  2, -1,  1,  0,  1,  3,  0,  0,  0,\n",
       "        2,  1, -1,  1,  1,  1,  1,  0,  2,  2,  2,  3, -1,  3,  3,  1,  3,\n",
       "       -1,  3,  3,  0,  2,  1,  1,  0,  2,  3,  1,  1,  2,  1,  2,  4,  3,\n",
       "        3,  1,  1,  1,  2,  1,  1,  2,  3,  1,  1,  0,  3,  4,  0,  6, -1,\n",
       "        2,  2,  0, -1,  1, -1,  1, -1,  2,  1,  1,  3,  0,  1,  1,  3,  1,\n",
       "        2,  1,  2,  3,  0,  1,  2,  0,  3,  1,  0,  1, -1,  1,  3, -1,  2,\n",
       "        3,  0,  1,  0,  0,  3,  3,  0,  0,  0,  3,  2,  1,  2,  1,  2,  0,\n",
       "        1,  0,  3,  6,  3,  2,  2,  1,  3, -1,  1,  1,  0, -1,  0,  0,  1,\n",
       "        3,  3,  6,  6,  1,  1,  1,  1,  1,  1,  0,  1,  3,  1,  3,  0,  1,\n",
       "        0,  1,  0,  0,  1,  3,  0, -1,  2,  1,  1,  0,  1,  1,  1,  1,  3,\n",
       "        6,  2,  0,  3,  0,  0,  0,  3,  0,  2,  3,  0,  3,  3,  0,  3,  6,\n",
       "        3, -1,  1,  1,  6,  3,  1,  0,  0,  2,  3,  3, -1,  0,  0,  3,  2,\n",
       "        1, -1,  1,  0, -1,  1,  3,  1,  1,  3, -1,  1,  1,  1,  0,  3, -1,\n",
       "       -1, -1,  1,  3,  3,  3,  0,  0,  0,  2,  3,  0,  2,  2,  3,  1,  2,\n",
       "        3,  1, -1,  1,  3,  3,  2,  1, -1,  1,  3,  1,  0,  0,  3,  2,  3,\n",
       "        3,  3,  1,  2,  0,  6,  3,  0,  1,  6,  1,  0, -1,  1,  1,  1,  3,\n",
       "        3, -1,  3,  0,  1,  1,  2,  3,  1,  3,  0,  3, -1,  2,  1,  3,  0,\n",
       "        1,  1,  2,  2,  0, -1,  0, -1,  3,  1,  1,  1,  0,  2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label = -1 if the instance is considered an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that DBSCAN does not have a `predict()` method, but we can use other classifiers with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=50, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other clustering algorithms\n",
    "\n",
    "More clustering algorithms and how they work:\n",
    "\n",
    "* **Agglomerative clustering**: at each iteration agglomerative clustering connects the nearest pair of clusters (starting with individual instances)  \n",
    "\n",
    "* **Birch**: it builds a tree structure during training containing just enough information to quickly assign each new instance to a cluster, without having to store all the instances in the tree  \n",
    "\n",
    "* **Mean-shift**: it works by placing a circle centered on each instance, then for each circle it computes the mean of all the instances located within it, and it shifts the circle so that it is centered on the mean. Next, it iterates this mean-shift step until all the circles stop moving (i.e., until each of them is centered on the mean of the instances it contains)  \n",
    "\n",
    "* **Affinity propagation**: instances vote for similar instances to be their representatives, and once the algorithm converges, each representative and its voters form a cluster   \n",
    "\n",
    "* **Spectral clustering**: takes a similarity matrix between the instances and creates a low-dimensional embedding from it (i.e., it reduces its dimension‐ ality), then it uses another clustering algorithm in this low-dimensional space (Scikit-Learn’s implementation uses K-Means)\n",
    "\n",
    "Brief overview of their pros and cons:\n",
    "\n",
    "Algorithm    | Pros          | Cons\n",
    "------------ | ------------- | -------------\n",
    "**Agglomerative clustering** | Scales very well to large numbers of instances or clusters | Need a connectivity matrix to scale well\n",
    "| Captures clusters of various shapes | \n",
    "| Produces flexible and informative tree | \n",
    "| Can be used with any pairwise distance | \n",
    "**Birch** | Designed specifically for large datasets | Work best with few features (<20)\n",
    "| Faster than K-means |\n",
    "**Mean-shift** |  It can find any number of clusters of any shape | Tends to split clusters irregularly when they have internal density variations \n",
    "| Only one hyperparameter (radius of the circles, i.e. bandwidth) | $O(m^2)$\n",
    "**Affinity propagation** | Can detect any number of clusters of different sizes | $O(m^2)$\n",
    "**Spectral clustering** | Can capture complex cluster structures | Does not scale well \n",
    "| Can be used to cut graphs (e.g. to identify clusters of friends on a social network) | Does not behave well with clusters of different sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixtures\n",
    "\n",
    "A **Gaussian mixture model** (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown.\n",
    "\n",
    "There are several GMM variants. In the most simple, we need to know in advance the number $k$ of gaussian distributions. The dataset $X$ is then assumed to have been generated through:\n",
    "\n",
    "* Picking a random cluster for each instance. The probability of choosing a cluster is defined by its weight. \n",
    "* After the instance has been assigned to the cluster, its location is sampled randomly from the Gaussian distribution. \n",
    "\n",
    "Let's use a diagram to explain this model more rigorously:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GMM](images/9.1_GMM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagram: \n",
    "* **Random variables** = circles \n",
    "* **Fixed values** = squares\n",
    "* **Plates** (content repeated several times) = large rectangles\n",
    "* **Plates repetition** = bottom right numbers ($m$ and $k$)\n",
    "\n",
    "Process:\n",
    "* Each variable $z^{(i)}$ is drawn from the _categorical distribution_ with weights $\\phi$ \n",
    "* Each variable $x^{(i)}$ is drawn from the normal distribution with the mean and covariance matrix defined by its cluster $z^{(i)}$\n",
    "* Solid arrows represent conditional dependencies\n",
    "* The non-solid arrow represent a switch: depending on the value of $z^{(i)}$, the instance $x^{(i)}$ will be sampled from a different Gaussian distribution\n",
    "* Shaded nodes indicate known variables (our random variables $x^{(i)}$ or **observed variables**). In contrast, $z^{(i)}$ are generally referred to as **latent variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it work in code? \n",
    "\n",
    "Given the dataset $X$, we start by estimating the weights $\\phi$ and the distribution parameters $\\mu^{(1)}$ to $\\mu^{(k)}$ and $\\Sigma^{(1)}$ to $\\Sigma^{(k)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=100,\n",
       "                means_init=None, n_components=3, n_init=10,\n",
       "                precisions_init=None, random_state=None, reg_covar=1e-06,\n",
       "                tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
       "                weights_init=None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm = GaussianMixture(n_components=3, n_init=10)\n",
    "gm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58992626, 0.20389046, 0.20618327])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50343194,  0.24946437],\n",
       "       [ 1.74701114, -0.05963289],\n",
       "       [-0.74068587,  0.565968  ]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.170685  , -0.10154005],\n",
       "        [-0.10154005,  0.28660015]],\n",
       "\n",
       "       [[ 0.04901551,  0.0596332 ],\n",
       "        [ 0.0596332 ,  0.089247  ]],\n",
       "\n",
       "       [[ 0.05481343,  0.06252493],\n",
       "        [ 0.06252493,  0.08706628]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This relies on **Expectation-Maximization** (EM) which initialize clusters randomly, repeats two steps until convergence first assigning instances to clusters (this is called the _expectation_\n",
    "step) then updating the clusters (this is called the _maximization_ step).\n",
    "\n",
    "EM uses soft cluster assignments, with the probabilities that certain instances belong to a certain class called **responsibilities**. \n",
    "\n",
    "EM can be thought as a generalization of K-means, and just like K-means, it may also not converge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.converged_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took a while! We can now assign instances using hard or soft clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 2, 1, 0, 0,\n",
       "       0, 1, 2, 2, 0, 0, 2, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 2, 1, 0,\n",
       "       0, 2, 2, 0, 0, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 1, 0, 0, 1, 2, 0, 1,\n",
       "       2, 0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 2, 1, 1, 0, 0, 0, 1, 2,\n",
       "       0, 2, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 0,\n",
       "       0, 2, 0, 1, 0, 0, 2, 2, 0, 0, 0, 0, 1, 0, 1, 0, 0, 2, 0, 0, 0, 0,\n",
       "       2, 2, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 2, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       1, 0, 0, 2, 1, 0, 2, 1, 0, 2, 1, 0, 0, 2, 2, 2, 0, 0, 2, 0, 0, 0,\n",
       "       1, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0,\n",
       "       0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 1, 0,\n",
       "       0, 2, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2,\n",
       "       2, 1, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 2, 2, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 2, 0, 0, 2, 2, 0, 0,\n",
       "       1, 0, 0, 2, 0, 2, 1, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 0, 2,\n",
       "       2, 2, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 2, 2,\n",
       "       0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 2,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 2, 0, 0, 1, 0, 0, 0, 2, 1,\n",
       "       2, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0,\n",
       "       2, 1, 2, 1, 0, 1, 0, 2, 2, 0, 0, 1, 0, 2, 1, 1, 0, 1, 2, 0, 2, 2,\n",
       "       1, 1, 0, 1, 2, 1, 2, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 2, 0, 2, 0, 1,\n",
       "       0, 1, 1, 2, 0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2,\n",
       "       1, 0, 0, 2, 0, 0, 2, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       2, 2, 0, 0, 0, 1, 2, 2, 0, 2, 0, 2, 0, 0, 0, 1, 1, 0, 2, 0, 0, 0,\n",
       "       2, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 2, 1, 1, 2, 2, 0, 0, 1, 0, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 1, 0, 2, 0, 0, 2, 0, 0, 0, 2,\n",
       "       0, 0, 0, 2, 0, 2, 0, 1, 2, 0, 0, 2, 1, 1, 0, 0, 0, 2, 2, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 2, 2, 0, 0, 2, 0,\n",
       "       0, 0, 1, 2, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 2, 0, 1, 0, 0, 1, 1, 1, 0, 2, 2, 0, 0, 2, 2, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0,\n",
       "       0, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2, 1, 0, 1, 1, 0, 0, 0, 0, 1, 2, 2,\n",
       "       0, 2, 0, 0, 2, 2, 0, 1, 2, 2, 0, 2, 0, 2, 0, 0, 1, 2, 0, 1, 0, 2,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 2, 1, 1, 0, 0, 1, 1, 1, 0, 0, 2, 0, 2,\n",
       "       0, 1, 2, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 1,\n",
       "       0, 2, 2, 2, 0, 2, 0, 1, 0, 0, 0, 0, 1, 2, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 2, 2, 1, 2, 0, 0, 2, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 2, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 2,\n",
       "       0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 1, 0, 1, 0, 2, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0,\n",
       "       2, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 1, 0, 1, 2, 1, 0, 0, 2, 0,\n",
       "       0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 0, 1, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 2, 2, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict(X) #hard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99999985e-001, 1.48426455e-008, 7.43438072e-127],\n",
       "       [1.48779942e-004, 9.99851220e-001, 1.22383565e-183],\n",
       "       [1.97398904e-004, 7.03346705e-211, 9.99802601e-001],\n",
       "       ...,\n",
       "       [9.16732532e-003, 1.51525827e-213, 9.90832675e-001],\n",
       "       [4.82020053e-003, 9.95179799e-001, 1.76295735e-199],\n",
       "       [1.00000000e+000, 7.41447484e-051, 2.83589281e-046]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gm.predict_proba(X) #soft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are many dimensions, or many clusters, or few instances, EM can struggle to converge to the optimal solution.\n",
    "\n",
    "One way to \"help the algorithm\" is to limit the number of parameters to learn, e.g. limiting the range of shapes and orientations that the clusters can have. This can be achieved by imposing constraints on the covariance matrices. To do this, just set the covariance_type hyperparameter to one of the following values:\n",
    "\n",
    "* `spherical`: spherical clusters, different diameters (variances) - $O(kmn)$\n",
    "* `diag`: any ellipsoidal shape of any size, but the ellipsoid’s axes must be parallel to the coordinate axes (diagonal covariance matrix) - $O(kmn)$\n",
    "* `tied`: same ellipsoidal shape, size and orientation (same covariance matrix) - $O(kmn^2 + kn^3)$\n",
    "* `full`: default - $O(kmn^2 + kn^3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection using Gaussian Mixtures\n",
    "\n",
    "Anomaly detection (also called _outlier detection_) is the task of detecting instances that deviate strongly from the norm.\n",
    "\n",
    "Using GMM for anomaly detection means considering any instance in a low-density region an anomaly, after defining an appropriate threshold. \n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = gm.score_samples(X)\n",
    "density_threshold = np.percentile(densities, 4) # 4th percentile threshold\n",
    "anomalies = X[densities < density_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting the number of clusters for GMM\n",
    "\n",
    "Unfortunately, inertia / silhouette scores are not useful with non spherical / different size clusters. Instead, we will find the model that minimizes a certain _theoretical information criterion_, for example:\n",
    "\n",
    "1. **Bayesian information criterion** (BIC) = $log(m)p - 2log(\\hat{L})$\n",
    "\n",
    "2. **Akaike information criterion** (AIC) = $2p - 2log(\\hat{L})$\n",
    "\n",
    "$m$ = number of instances  \n",
    "$p$ = number of parameters learned by the model  \n",
    "$\\hat{L}$ = maximized value of the _likelihood function_ of the model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a statistical model with parameters $\\theta$:\n",
    "\n",
    "* Probability = how plausible a future outcome $x$ is (knowing $\\theta$)\n",
    "* Likelihood = how plausible a particular set of parameter values $\\theta$ are (knowing $x$) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: It is important to understand that the likelihood function is not a probability distribution: if you integrate a probability distribution over all possible values of $x$, you always get 1, but if you integrate the likelihood function over all possible values of $\\theta$, the result can be any positive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a dataset $X$, a common task is to try to estimate the most likely values for the model parameters. To do this, we will need to maximize the likelihood function given $X$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Gaussian Mixture Models\n",
    "\n",
    "Rather than manually searching for the optimal number of clusters, it is possible to use the `BayesianGaussianMixture` class which can give weights to (~)0 to unnecessary clusters. \n",
    "\n",
    "Just set `n_components` (number of clusters) > what we expect to be the optimal number of clusters\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BayesianGaussianMixture(covariance_prior=None, covariance_type='full',\n",
       "                        degrees_of_freedom_prior=None, init_params='kmeans',\n",
       "                        max_iter=100, mean_precision_prior=None,\n",
       "                        mean_prior=None, n_components=10, n_init=10,\n",
       "                        random_state=42, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "                        verbose_interval=10, warm_start=False,\n",
       "                        weight_concentration_prior=None,\n",
       "                        weight_concentration_prior_type='dirichlet_process')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bgm.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15, 0.1 , 0.12, 0.12, 0.12, 0.15, 0.11, 0.13, 0.  , 0.  ])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(bgm.weights_, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the cluster parameters (including the weights, means and covariance matrices) are not treated as fixed model parameters anymore, but as latent random variables, like the cluster assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BGMM](images/9.1_BGMM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior knowledge about the latent variables **$z$** can be encoded in a probability distribution $p(z)$ called the **prior**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our probabilities using a renowned formula (and one of my favourite equations), the **Bayes' theorem**:\n",
    "\n",
    "$\\displaystyle p(z|x) = Posterior = \\frac{\\text{Likelihood x Prior}}{Evidence} = \\frac{p(X|z)p(z)}{p(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, in our equation the denominator is hard to obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(x) = \\int p(X|z)p(z)dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several approaches to solve this, one of them being _variational inference_, which picks a family of distributions $q(z;\\lambda)$ where $\\lambda$ are the _variational parameters_ which are optimized to approximate $p(z|X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture models work great on clusters with ellipsoidal shapes, but not so well on more irregular shapes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Anomaly Detection and Novelty Detection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Fast-MCD** (Minimum Covariance Determinant): useful for outlier detection. It assumes that the dataset has been generated by a Gaussian distribution, with outliers not from the same distribution. When it estimates the parameters of the Gaussian distribution it ignores instances that are most likely outliers.\n",
    "\n",
    "* **Isolation forest**: especially efficient in high-dimensional datasets. It builds a Random Forest in which every Decision Tree grows randomly: at each node, it picks a feature randomly, then it picks a random threshold value (between min and max) to split the dataset in two. It keeps splitting the dataset down to individual instances, and identifies outliers as instances for which less steps are needed to get to \"isolation\".\n",
    "\n",
    "* **Local outlier factor** (LOF): it compares the density of instances around a given instance to the density around its neighbors. An anomaly is often more isolated than its $k$ nearest neighbors.\n",
    "\n",
    "* **One-class SVM**: better suited for novelty detection. Since we just have one class of instances, the one-class SVM algorithm tries to separate the instances in high-dimensional space from the origin, finding a small region that encompasses all the instances. If a new instance does not fall within this region, it is an anomaly. Works well with high-dimensional datasets but it does not scale very well to large datasets.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
