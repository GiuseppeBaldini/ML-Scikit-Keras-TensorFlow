{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating the result of different predictors is called **Ensemble Learning**. Similarly to the phenomenon known as _wisdom of the crowd_, these aggregated models often turn out to be more effective than any individual one. \n",
    "\n",
    "**Note**: Ensemble methods work best when the predictors are as independent from one another as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifiers\n",
    "\n",
    "A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes (**hard voting**) or predict the class with the highest class probability, averaged over all the individual classifiers (**soft voting**) assuming all the classifiers outputs are probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging and Pasting\n",
    "\n",
    "The method we discussed above involves using different classifiers on the same dataset. Another approach is to use the same training algorithm for every\n",
    "predictor, but to train them on different random subsets of the training set. \n",
    "\n",
    "This second method has two variants:\n",
    "1. **Bagging** (with replacement)\n",
    "2. **Pasting** (without replacement)\n",
    "\n",
    "Generally, the net result is that the ensemble has a similar bias but a lower variance than a single predictor trained on the original training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out-of-bag Evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. Usually this means around 37% of instances never sampled (they are left **out-of-bag** so to speak). We can therefore use them to evaluate our predictors, since our predictor has never seen them in training.  \n",
    "\n",
    "Simply add `oob_score=True` to the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Patches and Random Subspaces\n",
    "\n",
    "Sometimes, especially when dealing with high dimensionality, it may be helpful to sample a subset of **features** rather than samples. \n",
    "* Sampling both training instances and features is called the **Random Patches** method\n",
    "* Keeping all training instances and sampling features is called the **Random Subspaces** method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
